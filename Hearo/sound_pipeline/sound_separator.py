#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sound Separator Module
- separator.py ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„
- ê°ë„ ì •ë³´ë¥¼ ë°±ì—”ë“œì— ì „ë‹¬
- ìŒì› ë¶„ë¦¬ ë° ë¶„ë¥˜ ê¸°ëŠ¥
"""

import os
import sys
import time
import warnings
import numpy as np
import torch
import torch.nn.functional as F
import torchaudio
import requests
from typing import List, Tuple, Optional, Dict, Any

# separator.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆë“¤ import
warnings.filterwarnings("ignore")
torch.set_num_threads(4)

try:
    from transformers import ASTFeatureExtractor, ASTForAudioClassification
except ImportError:
    print("Warning: transformers not available. Sound separation will be disabled.")
    ASTFeatureExtractor = None
    ASTForAudioClassification = None

# =========================
# Config (separator.pyì™€ ë™ì¼)
# =========================
SR = 16000
WIN_SEC = 4.096
ANCHOR_SEC = 0.512
L_FIXED = int(round(WIN_SEC * SR))

NORMALIZE_TARGET_PEAK = 0.95
RESIDUAL_CLIP_THR = 0.0005

USE_ADAPTIVE_STRATEGY = True
FALLBACK_THRESHOLD = 0.1

MASK_SIGMOID_CENTER = 0.6
MASK_SIGMOID_SLOPE = 20.0

N_FFT, HOP, WINLEN = 400, 160, 400
WINDOW = torch.hann_window(WINLEN)
EPS = 1e-10

N_MELS = 128

SMOOTH_T = 19
ALPHA_ATT = 0.60
BETA_PUR = 1.50
W_E = 0.40
TOP_PCT_CORE_IN_ANCHOR = 0.50

OMEGA_Q_CONSERVATIVE = 0.9
OMEGA_Q_AGGRESSIVE = 0.7
OMEGA_DIL = 2
OMEGA_MIN_BINS = 5

AST_FREQ_QUANTILE_CONSERVATIVE = 0.7
AST_FREQ_QUANTILE_AGGRESSIVE = 0.4

DANGER_IDS = {0,396, 397, 398, 399, 400, 426, 436}
HELP_IDS = {23, 14, 354, 355, 356, 359}
WARNING_IDS = {288, 364, 388, 389, 390, 439, 391, 392, 393, 395, 440, 441, 443, 456, 469, 470, 478, 479}

PRES_Q = 0.20
PRES_SMOOTH_T = 9

USED_THRESHOLD = 0.65
USED_DILATE_MS = 80
ANCHOR_SUPPRESS_MS = 200
ANCHOR_SUPPRESS_BASE = 0.6

MAX_PASSES = 3
MIN_ERATIO = 0.005  # ë” ì•½í•œ ì†Œë¦¬ë„ ë¶„ë¦¬í•˜ë„ë¡ ì„ê³„ê°’ ë‚®ì¶¤

# ë¶„ë¦¬ ê°•ë„ ì¡°ì • (ë‹¤ì¤‘ ì†Œë¦¬ ë¶„ë¦¬ ê°œì„ )
# Threshold ë§ˆìŠ¤í‚¹ ì„¤ì •
MASK_THRESHOLD = 0.6        # ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í‚¹ (0.3 ì´ìƒì´ë©´ 1, ë¯¸ë§Œì´ë©´ 0)
MASK_SOFTNESS = 0.1         # ë¶€ë“œëŸ¬ìš´ ì „í™˜ì„ ìœ„í•œ ë²”ìœ„ (threshold Â± softness)
USE_HARD_THRESHOLD = False  # Trueë©´ ì™„ì „í•œ í•˜ë“œ threshold, Falseë©´ ë¶€ë“œëŸ¬ìš´ threshold

# ë‹¤ì–‘í•œ threshold ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)
THRESHOLD_PRESETS = {
    "conservative": 0.5,    # ë³´ìˆ˜ì  ë¶„ë¦¬ (ë†’ì€ ì„ê³„ê°’)
    "balanced": 0.3,        # ê· í˜•ì¡íŒ ë¶„ë¦¬
    "aggressive": 0.2,      # ê³µê²©ì  ë¶„ë¦¬ (ë‚®ì€ ì„ê³„ê°’)
    "very_aggressive": 0.1  # ë§¤ìš° ê³µê²©ì  ë¶„ë¦¬
}
CURRENT_THRESHOLD_PRESET = "balanced"  # í˜„ì¬ ì‚¬ìš©í•  preset
ALPHA_ATT = 0.30            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ë” ë³´ìˆ˜ì ì¸ ë¶„ë¦¬)
BETA_PUR = 0.8              # ìˆœë„ ê°€ì¤‘ì¹˜ (ë” ë³´ìˆ˜ì ì¸ ë¶„ë¦¬)
W_E = 0.20                  # ì—ë„ˆì§€ ê°€ì¤‘ì¹˜ (ì”ì—¬ë¬¼ì— ë” ë§ì€ ì—ë„ˆì§€ ë³´ì¡´)

# ì”ì—¬ë¬¼ ì¦í­ ì„¤ì •
RESIDUAL_AMPLIFY = True     # ì”ì—¬ë¬¼ ì¦í­ í™œì„±í™”
RESIDUAL_GAIN = 2.0         # ì”ì—¬ë¬¼ ì¦í­ ë°°ìˆ˜ (2ë°°)
RESIDUAL_MAX_GAIN = 4.0     # ìµœëŒ€ ì¦í­ ë°°ìˆ˜ (4ë°°)

# Backend API
USER_ID = 6
BACKEND_URL = "http://13.238.200.232:8000/sound-events/"

# =========================
# Utility Functions (separator.pyì—ì„œ ê°€ì ¸ì˜´)
# =========================
def norm01(x: torch.Tensor) -> torch.Tensor:
    return (x - x.min()) / (x.max() - x.min() + 1e-8)

def ensure_odd(k: int) -> int:
    return k + 1 if (k % 2 == 0) else k

def smooth1d(x: torch.Tensor, k: int) -> torch.Tensor:
    if k <= 1: return x
    ker = torch.ones(k, device=x.device) / k
    return F.conv1d(x.view(1,1,-1), ker.view(1,1,-1), padding=k//2).view(-1)

def to_np(x: torch.Tensor) -> np.ndarray:
    return x.detach().cpu().numpy()

def align_len_1d(x: torch.Tensor, T: int, device=None, mode="linear"):
    if device is None: device = x.device
    xv = x.to(device).view(1,1,-1).float()
    if xv.size(-1) == T:
        out = xv.view(-1)
    else:
        out = F.interpolate(xv, size=T, mode=mode, align_corners=False).view(-1)
    return out.clamp(0,1)

def soft_sigmoid(x: torch.Tensor, center: float, slope: float, min_val: float = 0.0) -> torch.Tensor:
    sig = torch.sigmoid(slope * (x - center))
    return min_val + (1.0 - min_val) * sig

def amplify_residual(residual: np.ndarray, gain: float = 2.0, max_gain: float = 4.0) -> np.ndarray:
    """ì”ì—¬ë¬¼ ì¦í­ (í´ë¦¬í•‘ ë°©ì§€)"""
    try:
        # í˜„ì¬ RMS ê³„ì‚°
        current_rms = np.sqrt(np.mean(residual ** 2))
        if current_rms < 1e-8:  # ë„ˆë¬´ ì‘ì€ ê²½ìš°
            return residual
        
        # ì¦í­ ì ìš©
        amplified = residual * gain
        
        # í´ë¦¬í•‘ ë°©ì§€ (ìµœëŒ€ ì¦í­ ì œí•œ)
        max_amplified_rms = current_rms * max_gain
        current_amplified_rms = np.sqrt(np.mean(amplified ** 2))
        
        if current_amplified_rms > max_amplified_rms:
            # ìµœëŒ€ ì¦í­ìœ¼ë¡œ ì œí•œ
            amplified = amplified * (max_amplified_rms / current_amplified_rms)
        
        # -1.0 ~ 1.0 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        amplified = np.clip(amplified, -1.0, 1.0)
        
        return amplified
        
    except Exception as e:
        print(f"[Separator] Residual amplification error: {e}")
        return residual


class SoundSeparator:
    def __init__(self, model_name: str = "MIT/ast-finetuned-audioset-10-10-0.4593", 
                 device: str = "auto", backend_url: str = BACKEND_URL):
        """
        Sound Separator ì´ˆê¸°í™”
        
        Args:
            model_name: AST ëª¨ë¸ ì´ë¦„
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (auto/cpu/cuda)
            backend_url: ë°±ì—”ë“œ API URL
        """
        self.model_name = model_name
        self.backend_url = backend_url
        
        # Device ì„¤ì •
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        self.extractor = None
        self.ast_model = None
        self.mel_fb_m2f = None
        self.is_available = False
        
        # ë¶„ë¦¬ ê´€ë ¨ ìºì‹œ
        self.attention_cache = {}
        self.freq_attention_cache = {}
        self.cls_head_cache = {}
        self.spectrogram_cache = {}
        
        self._initialize_model()
    
    def _initialize_model(self):
        """AST ëª¨ë¸ ì´ˆê¸°í™” (ì‹¤ì „ìš©)"""
        try:
            if ASTFeatureExtractor is None or ASTForAudioClassification is None:
                print("[Separator] âŒ Transformers not available - ì‹¤ì „ ëª¨ë“œì—ì„œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤!")
                self.is_available = False
                return
            
            print(f"[Separator] Loading AST model: {self.model_name}")
            print(f"[Separator] Device: {self.device}")
            
            self.extractor = ASTFeatureExtractor.from_pretrained(self.model_name)
            self.ast_model = ASTForAudioClassification.from_pretrained(self.model_name).to(self.device)
            self.ast_model.eval()
            
            # Mel filterbank ìƒì„±
            self.mel_fb_m2f = torchaudio.transforms.MelScale(n_mels=N_MELS, sample_rate=SR, n_stft=N_FFT//2+1).fb
            
            self.is_available = True
            print("[Separator] âœ… AST model loaded successfully")
            
        except Exception as e:
            print(f"[Separator] âŒ Model loading error: {e}")
            print("[Separator] ì‹¤ì „ ëª¨ë“œì—ì„œëŠ” ëª¨ë¸ ë¡œë”©ì´ í•„ìˆ˜ì…ë‹ˆë‹¤!")
            self.is_available = False
    
    def _get_sound_type(self, class_id: int) -> str:
        """í´ë˜ìŠ¤ IDë¥¼ ì†Œë¦¬ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        if class_id in DANGER_IDS:
            return "danger"
        elif class_id in HELP_IDS:
            return "help"
        elif class_id in WARNING_IDS:
            return "warning"
        else:
            return "other"
    
    def _calculate_decibel(self, audio: np.ndarray) -> Tuple[float, float, float]:
        """dB ë ˆë²¨ ê³„ì‚°"""
        try:
            # ì˜¤ë””ì˜¤ ë°ì´í„° ê²€ì¦
            if len(audio) == 0:
                return -np.inf, -np.inf, -np.inf
            
            # RMS ê³„ì‚°
            rms = np.sqrt(np.mean(audio**2))
            
            if rms <= 0:
                return -np.inf, -np.inf, -np.inf
            
            # dB ë³€í™˜ (20 * log10(rms))
            db = 20 * np.log10(rms)
            
            # ìœ íš¨í•œ dB ê°’ì¸ì§€ í™•ì¸
            if np.isnan(db) or np.isinf(db):
                return -np.inf, -np.inf, -np.inf
            
            # min, max dB ê³„ì‚° (ê°„ë‹¨í•œ ë°©ë²•)
            audio_abs = np.abs(audio)
            audio_abs = audio_abs[audio_abs > 1e-10]  # ë§¤ìš° ì‘ì€ ê°’ ì œì™¸
            
            if len(audio_abs) > 0:
                db_min = 20 * np.log10(np.min(audio_abs))
                db_max = 20 * np.log10(np.max(audio_abs))
            else:
                db_min = db_max = db
            
            return db_min, db_max, db
            
        except Exception as e:
            print(f"[Separator] dB calculation error: {e}")
            return -np.inf, -np.inf, -np.inf
    
    def _prepare_audio_for_classification(self, audio_raw: np.ndarray) -> np.ndarray:
        """
        ë¶„ë¥˜ìš© ì •ê·œí™”ëœ ì˜¤ë””ì˜¤ ë°ì´í„° ì¤€ë¹„
        
        Args:
            audio_raw: ì›ë³¸ int16 ì˜¤ë””ì˜¤ ë°ì´í„°
            
        Returns:
            ì •ê·œí™”ëœ float32 ì˜¤ë””ì˜¤ ë°ì´í„°
        """
        try:
            # int16ì„ float32ë¡œ ì •ê·œí™” (-1.0 ~ 1.0 ë²”ìœ„)
            audio_normalized = audio_raw.astype(np.float32) / 32767.0
            
            # 10ì´ˆë¡œ íŒ¨ë”©
            target_len = int(10.0 * SR)
            if len(audio_normalized) < target_len:
                audio_padded = np.zeros(target_len, dtype=np.float32)
                audio_padded[:len(audio_normalized)] = audio_normalized
                return audio_padded
            else:
                return audio_normalized[:target_len]
                
        except Exception as e:
            print(f"[Separator] Error preparing audio for classification: {e}")
            return audio_raw.astype(np.float32) / 32767.0
    
    def _calculate_decibel_from_raw(self, audio_raw: np.ndarray) -> Tuple[float, float, float]:
        """
        Sound Triggerì˜ _calculate_db_levelê³¼ ì™„ì „íˆ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ë°ì‹œë²¨ ê³„ì‚°
        
        Args:
            audio_raw: ì›ë³¸ int16 ì˜¤ë””ì˜¤ ë°ì´í„° (ëª¨ë…¸)
            
        Returns:
            (db_min, db_max, db_mean)
        """
        try:
            if len(audio_raw) == 0:
                return -np.inf, -np.inf, -np.inf
            
            # Sound Triggerì˜ _calculate_db_levelê³¼ ì™„ì „íˆ ë™ì¼í•œ ë¡œì§
            # ëª¨ë…¸ ë°ì´í„°ì´ë¯€ë¡œ channels <= 1 ì¡°ê±´ì— í•´ë‹¹
            audio_data = audio_raw.astype(np.float32)
            
            # RMS ê³„ì‚° (Sound Triggerì™€ ë™ì¼)
            rms = np.sqrt(np.mean(audio_data**2))
            
            if rms == 0:
                return -np.inf, -np.inf, -np.inf
            
            # dB ë³€í™˜ (20 * log10(rms)) - Sound Triggerì™€ ë™ì¼
            if rms > 0:
                db = 20 * np.log10(rms)
                
                # ìœ íš¨í•œ dB ê°’ì¸ì§€ í™•ì¸ (Sound Triggerì™€ ë™ì¼)
                if np.isnan(db) or np.isinf(db):
                    return -np.inf, -np.inf, -np.inf
                
                # min, max dB ê³„ì‚° (ê°„ë‹¨í•œ ë°©ë²•)
                audio_abs = np.abs(audio_data)
                audio_abs = audio_abs[audio_abs > 1e-10]  # ë§¤ìš° ì‘ì€ ê°’ ì œì™¸
                
                if len(audio_abs) > 0:
                    db_min = 20 * np.log10(np.min(audio_abs))
                    db_max = 20 * np.log10(np.max(audio_abs))
                    
                    # ìœ íš¨ì„± ê²€ì‚¬
                    if np.isnan(db_min) or np.isinf(db_min):
                        db_min = db
                    if np.isnan(db_max) or np.isinf(db_max):
                        db_max = db
                else:
                    db_min = db_max = db
                
                return db_min, db_max, db
            else:
                return -np.inf, -np.inf, -np.inf
            
        except Exception as e:
            print(f"[Separator] Raw dB calculation error: {e}")
            import traceback
            traceback.print_exc()
            return -np.inf, -np.inf, -np.inf
    
    def _calculate_decibel_simple(self, audio: np.ndarray) -> Tuple[float, float, float]:
        """ë¼ì¦ˆë² ë¦¬ íŒŒì´ í˜¸í™˜ dB ê³„ì‚° ë°©ë²•"""
        try:
            if len(audio) == 0:
                return -np.inf, -np.inf, -np.inf
            
            # ì•ˆì „í•œ ìë£Œí˜• ë³€í™˜ (ë¼ì¦ˆë² ë¦¬ íŒŒì´ í˜¸í™˜)
            if audio.dtype == np.int16:
                # int16ì„ float64ë¡œ ë³€í™˜í•˜ì—¬ ì •ë°€ë„ í–¥ìƒ
                audio_float = audio.astype(np.float64)
            elif audio.dtype == np.float32:
                audio_float = audio.astype(np.float64)
            elif audio.dtype == np.float64:
                audio_float = audio.copy()
            else:
                audio_float = audio.astype(np.float64)
            
            # ë°ì´í„° ê²€ì¦
            if np.all(audio_float == 0):
                return -np.inf, -np.inf, -np.inf
            
            # RMS ê³„ì‚° (Sound Triggerì™€ ë™ì¼í•œ ë°©ì‹)
            rms = np.sqrt(np.mean(audio_float**2))
            
            if rms <= 0:
                return -np.inf, -np.inf, -np.inf
            
            # dB ë³€í™˜ (20 * log10(rms)) - Sound Triggerì™€ ë™ì¼
            db = 20 * np.log10(rms)
            
            # ìœ íš¨í•œ dB ê°’ì¸ì§€ í™•ì¸
            if np.isnan(db) or np.isinf(db):
                return -np.inf, -np.inf, -np.inf
            
            # min, max dB ê³„ì‚° (ì•ˆì „í•œ ë°©ë²•)
            audio_abs = np.abs(audio_float)
            # 0ì´ ì•„ë‹Œ ê°’ë“¤ë§Œ ì‚¬ìš©
            non_zero_mask = audio_abs > 1e-10
            audio_abs_nonzero = audio_abs[non_zero_mask]
            
            if len(audio_abs_nonzero) > 0:
                db_min = 20 * np.log10(np.min(audio_abs_nonzero))
                db_max = 20 * np.log10(np.max(audio_abs_nonzero))
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if np.isnan(db_min) or np.isinf(db_min):
                    db_min = db
                if np.isnan(db_max) or np.isinf(db_max):
                    db_max = db
            else:
                db_min = db_max = db
            
            # ìµœì¢… ê²€ì¦
            if np.isnan(db_min) or np.isinf(db_min) or np.isnan(db_max) or np.isinf(db_max) or np.isnan(db) or np.isinf(db):
                return -np.inf, -np.inf, -np.inf
            
            return db_min, db_max, db
            
        except Exception as e:
            print(f"[Separator] Simple dB calculation error: {e}")
            import traceback
            traceback.print_exc()
            return -np.inf, -np.inf, -np.inf
    
    def _send_to_backend(self, sound_type: str, sound_detail: str, decibel: float, angle: int) -> bool:
        """
        Send results to backend (including angle information)
        
        Args:
            sound_type: Sound type (danger/warning/help/other)
            sound_detail: Sound detail information
            decibel: dB level
            angle: Angle (0-359)
            
        Returns:
            Transmission success status
        """
        try:
            data = {
                "user_id": USER_ID,
                "sound_type": sound_type,
                "sound_detail": sound_detail,
                "angle": angle,  # Include angle information
                "occurred_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                "sound_icon": "string",
                "location_image_url": "string",
                "decibel": float(decibel),
            }
            
            headers = {
                'Content-Type': 'application/json',
                'User-Agent': 'SoundPipeline/1.0'
            }
            
            print(f"ğŸ”„ Sending to backend: {self.backend_url}")
            print(f"ğŸ“¤ Data: {data}")
            
            # Disable SSL warnings for testing
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            response = requests.post(
                self.backend_url, 
                json=data, 
                headers=headers,
                timeout=10.0,  # Increased timeout
                verify=False
            )
            
            if response.status_code == 200:
                print(f"âœ… Sent to backend: {sound_detail} ({sound_type}) at {angle}Â°")
                return True
            else:
                print(f"âŒ Backend error: {response.status_code}")
                print(f"âŒ Response: {response.text}")
                return False
                
        except requests.exceptions.ConnectTimeout:
            print(f"âŒ Backend connection timeout: {self.backend_url}")
            return False
        except requests.exceptions.ConnectionError as e:
            print(f"âŒ Backend connection error: {e}")
            return False
        except requests.exceptions.RequestException as e:
            print(f"âŒ Backend request error: {e}")
            return False
        except Exception as e:
            print(f"âŒ Unexpected error sending to backend: {e}")
            return False
    
    def _load_fixed_audio(self, path: str) -> np.ndarray:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ - Sound Triggerì™€ ë™ì¼í•œ ë°©ì‹"""
        try:
            import wave
            
            # Sound Triggerì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ WAV íŒŒì¼ ì½ê¸°
            with wave.open(path, 'rb') as wav_file:
                # WAV íŒŒì¼ ì •ë³´ í™•ì¸
                channels = wav_file.getnchannels()
                sample_width = wav_file.getsampwidth()
                framerate = wav_file.getframerate()
                n_frames = wav_file.getnframes()
                
                # Sound Triggerì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ì½ê¸°
                raw_audio = wav_file.readframes(n_frames)
                
                # Sound Triggerì™€ ë™ì¼í•œ int16 ë³€í™˜
                if sample_width == 2:  # 16-bit
                    # Sound Triggerì™€ ë™ì¼: np.frombuffer ì‚¬ìš©
                    audio_data = np.frombuffer(raw_audio, dtype=np.int16)
                elif sample_width == 1:  # 8-bit
                    audio_data = np.frombuffer(raw_audio, dtype=np.uint8).astype(np.int16) - 128
                else:
                    print(f"[Separator] Warning: Unsupported sample width: {sample_width}")
                    return np.zeros(L_FIXED, dtype=np.int16)
            
            # ë°ì´í„° ê²€ì¦
            if len(audio_data) == 0:
                print(f"[Separator] Warning: Empty audio data from {path}")
                return np.zeros(L_FIXED, dtype=np.int16)
            
            # 0 ë°ì´í„° ê²€ì¦
            if np.all(audio_data == 0):
                print(f"[Separator] Warning: All audio data is zero from {path}")
                return np.zeros(L_FIXED, dtype=np.int16)
            
            # Sound Triggerì™€ ë™ì¼í•œ ëª¨ë…¸ ë³€í™˜ ë°©ì‹
            if channels > 1:
                    # Sound Triggerì˜ _to_mono_int16ê³¼ ë™ì¼í•œ ë¡œì§
                    usable_len = (len(audio_data) // channels) * channels
                    if usable_len != len(audio_data):
                        audio_data = audio_data[:usable_len]
                    x = audio_data.reshape(-1, channels)
                    
                    # ì±„ë„ 5ê°€ ìˆìœ¼ë©´ ê·¸ ì±„ë„ë§Œ ì‚¬ìš© (Sound Triggerì™€ ë™ì¼)
                    if channels >= 6:
                        mono = x[:, 5].astype(np.int16)
                    else:
                        # ì¼ë°˜ ë§ˆì´í¬ ì±„ë„ í‰ê·  (ê°€ëŠ¥í•˜ë©´ ì•ìª½ 4ì±„ë„ë§Œ í‰ê· )
                        mic_cols = min(channels, 4)
                        mono = np.mean(x[:, :mic_cols], axis=1).astype(np.int16)
                    
                    audio_data = mono
                
            # ìƒ˜í”Œë§ ë ˆì´íŠ¸ ë³€í™˜ (ê°„ë‹¨í•œ ë¦¬ìƒ˜í”Œë§)
            if framerate != SR:
                # ê°„ë‹¨í•œ ë¦¬ìƒ˜í”Œë§ (ì„ í˜• ë³´ê°„)
                ratio = SR / framerate
                new_length = int(len(audio_data) * ratio)
                audio_data = np.interp(
                    np.linspace(0, len(audio_data), new_length),
                    np.arange(len(audio_data)),
                    audio_data.astype(np.float64)
                ).astype(np.int16)
                
            
            # ê³ ì • ê¸¸ì´ë¡œ ì¡°ì •
            if len(audio_data) >= L_FIXED:
                return audio_data[:L_FIXED]
            else:
                out = np.zeros(L_FIXED, dtype=np.int16)
                out[:len(audio_data)] = audio_data
                return out
                
        except Exception as e:
            print(f"[Separator] Error loading audio {path}: {e}")
            import traceback
            traceback.print_exc()
            return np.zeros(L_FIXED, dtype=np.int16)
    
    def _classify_audio(self, audio_normalized: np.ndarray) -> Tuple[str, str, int, float]:
        """
        ì˜¤ë””ì˜¤ ë¶„ë¥˜ (ì‹¤ì „ìš©) - ì •ê·œí™”ëœ ë°ì´í„° ì‚¬ìš©
        
        Args:
            audio_normalized: ì •ê·œí™”ëœ float32 ì˜¤ë””ì˜¤ ë°ì´í„° (-1.0 ~ 1.0)
            
        Returns:
            (class_name, sound_type, class_id, confidence)
        """
        if not self.is_available:
            print("[Separator] âŒ Model not available - ì‹¤ì „ ëª¨ë“œì—ì„œëŠ” ëª¨ë¸ì´ í•„ìˆ˜ì…ë‹ˆë‹¤!")
            return "Unknown", "other", 0, 0.0
        
        try:
            # ì´ë¯¸ ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©
            audio_float = audio_normalized.astype(np.float32)
            
            # 10ì´ˆë¡œ íŒ¨ë”© (ì´ë¯¸ _prepare_audio_for_classificationì—ì„œ ì²˜ë¦¬ë¨)
            target_len = int(10.0 * SR)
            if len(audio_float) < target_len:
                audio_padded = np.zeros(target_len, dtype=np.float32)
                audio_padded[:len(audio_float)] = audio_float
            else:
                audio_padded = audio_float[:target_len]
            
            feat = self.extractor(audio_padded, sampling_rate=SR, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.ast_model(input_values=feat["input_values"].to(self.device))
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1)
                predicted_class_id = logits.argmax(dim=-1).item()
                confidence = probabilities[0, predicted_class_id].item()
            
            class_name = self.ast_model.config.id2label[predicted_class_id]
            sound_type = self._get_sound_type(predicted_class_id)
            
            return class_name, sound_type, predicted_class_id, confidence
            
        except Exception as e:
            print(f"[Separator] âŒ Classification error: {e}")
            return "Unknown", "other", 0, 0.0
    
    def _save_separated_audio(self, audio: np.ndarray, class_name: str, sound_type: str, output_dir: str, suffix: str = "") -> str:
        """
        ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            audio: ì˜¤ë””ì˜¤ ë°ì´í„° (int16 ë˜ëŠ” float32)
            class_name: ë¶„ë¥˜ëœ í´ë˜ìŠ¤ ì´ë¦„
            sound_type: ì†Œë¦¬ íƒ€ì…
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(output_dir, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ + í´ë˜ìŠ¤ëª… + íƒ€ì…)
            import time
            timestamp = int(time.time())
            safe_class_name = "".join(c for c in class_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_class_name = safe_class_name.replace(' ', '_')
            
            filename = f"separated_{timestamp}_{safe_class_name}_{sound_type}{suffix}.wav"
            filepath = os.path.join(output_dir, filename)
            
            # int16 ë°ì´í„°ë¥¼ float32ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            if audio.dtype == np.int16:
                audio_float = audio.astype(np.float32) / 32767.0  # -1.0 ~ 1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
            else:
                audio_float = audio.astype(np.float32)
            
            # ì˜¤ë””ì˜¤ ì €ì¥
            torchaudio.save(filepath, torch.from_numpy(audio_float).unsqueeze(0), SR)
            
            print(f"[Separator] Separated audio saved: {filename}")
            return filepath
            
        except Exception as e:
            print(f"[Separator] Error saving separated audio: {e}")
            return None
    
    def process_audio(self, audio_file: str, angle: int, output_dir: str = None) -> Dict[str, Any]:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ë° ë¶„ë¥˜
        
        Args:
            audio_file: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            angle: ê°ë„ (0-359)
            output_dir: ë¶„ë¦¬ëœ ì†Œë¦¬ ì €ì¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"[Separator] Processing audio: {audio_file}")
        print(f"[Separator] Angle: {angle}Â°")
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ (ì›ë³¸ raw ë°ì´í„°)
            audio_raw = self._load_fixed_audio(audio_file)
            print(f"[Separator] Audio length: {len(audio_raw)/SR:.2f}s")
            
            # ë¶„ë¥˜ìš© ì •ê·œí™”ëœ ì˜¤ë””ì˜¤ ìƒì„±
            audio_normalized = self._prepare_audio_for_classification(audio_raw)
            
            # ë¶„ë¥˜ (ì •ê·œí™”ëœ ë°ì´í„° ì‚¬ìš©)
            class_name, sound_type, class_id, confidence = self._classify_audio(audio_normalized)
            
            # dB ê³„ì‚° (ì›ë³¸ raw ë°ì´í„° ì‚¬ìš© - Sound Triggerì™€ ë™ì¼í•œ ë°©ë²•)
            db_min, db_max, db_mean = self._calculate_decibel_from_raw(audio_raw)
            
            print(f"[Separator] Classified: {class_name} ({sound_type})")
            print(f"[Separator] Confidence: {confidence:.3f}")
            print(f"[Separator] Decibel: {db_mean:.1f} dB")
            
            # ìŒì› ë¶„ë¦¬ ì‹¤í–‰ (ìƒˆë¡œìš´ ê¸°ëŠ¥!)
            separated_sources = []
            separated_file = None
            if output_dir:
                if self.is_available:
                    print(f"[Separator] Starting source separation...")
                    separated_sources = self.separate_audio(audio_normalized, max_passes=MAX_PASSES)
                    
                    # ë¶„ë¦¬ëœ ì†Œë¦¬ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥
                    for i, source in enumerate(separated_sources):
                        if source['audio'] is not None:
                            source_file = self._save_separated_audio(
                                source['audio'], 
                                source['class_name'], 
                                source['sound_type'], 
                                output_dir,
                                suffix=f"_pass_{source['pass']}"
                            )
                            source['file'] = source_file
                            print(f"[Separator] Separated source {i+1}: {source['class_name']} ({source['sound_type']}) - {source['confidence']:.3f}")
                    
                    # ì²« ë²ˆì§¸ ë¶„ë¦¬ëœ ì†Œë¦¬ë¥¼ ê¸°ë³¸ separated_fileë¡œ ì„¤ì •
                    if separated_sources:
                        separated_file = separated_sources[0]['file']
                else:
                    # ë¶„ë¦¬ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì›ë³¸ ë°ì´í„° ì €ì¥
                    separated_file = self._save_separated_audio(audio_raw, class_name, sound_type, output_dir)
            
            # ë°±ì—”ë“œ ì „ì†¡ (other íƒ€ì… ì œì™¸)
            backend_success = False
            if sound_type != "other":
                backend_success = self._send_to_backend(sound_type, class_name, db_mean, angle)
            else:
                print(f"[Separator] Skipping backend send for 'other' type: {class_name}")
                backend_success = True
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                "success": True,
                "class_name": class_name,
                "sound_type": sound_type,
                "class_id": class_id,
                "confidence": confidence,
                "angle": angle,
                "decibel": {
                    "min": db_min,
                    "max": db_max,
                    "mean": db_mean
                },
                "backend_success": backend_success,
                "audio_file": audio_file,
                "separated_file": separated_file,
                "separated_sources": separated_sources,  # ìƒˆë¡œìš´ í•„ë“œ: ë¶„ë¦¬ëœ ì†Œë¦¬ë“¤
                "separation_enabled": self.is_available
            }
            
            return result
            
        except Exception as e:
            print(f"[Separator] Processing error: {e}")
            return {
                "success": False,
                "error": str(e),
                "audio_file": audio_file,
                "angle": angle
            }
    
    # =========================
    # Source Separation Logic (separator.pyì—ì„œ ê°€ì ¸ì˜´)
    # =========================
    
    def _get_cache_key(self, audio: np.ndarray) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return str(hash(audio.tobytes()))
    
    def _extract_and_cache_attention(self, audio: np.ndarray, T_out: int, F_out: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """AST ëª¨ë¸ í˜¸ì¶œí•˜ì—¬ ì‹œê°„/ì£¼íŒŒìˆ˜ attention mapê³¼ CLS features ì¶”ì¶œ ë° ìºì‹±"""
        cache_key = self._get_cache_key(audio)
        
        # ìºì‹œ í™•ì¸
        if cache_key in self.attention_cache and cache_key in self.cls_head_cache:
            freq_attn = self.freq_attention_cache.get(cache_key)
            if freq_attn is not None:
                return self.attention_cache[cache_key], freq_attn, self.cls_head_cache[cache_key]
        
        # int16 ë°ì´í„°ë¥¼ float32ë¡œ ë³€í™˜
        if audio.dtype == np.int16:
            audio = audio.astype(np.float32) / 32767.0
        elif audio.dtype != np.float32:
            audio = audio.astype(np.float32)
        
        # 10ì´ˆë¡œ íŒ¨ë”©
        target_len = int(10.0 * SR)
        if len(audio) < target_len:
            audio_padded = np.zeros(target_len, dtype=np.float32)
            audio_padded[:len(audio)] = audio
        else:
            audio_padded = audio[:target_len]
        
        feat = self.extractor(audio_padded, sampling_rate=SR, return_tensors="pt")
        
        # Mel ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì¶”ì¶œ (ìºì‹±ìš©)
        mel_spec = feat["input_values"].squeeze(0)  # [N_MELS, T]
        
        with torch.no_grad():
            outputs = self.ast_model(input_values=feat["input_values"].to(self.device), output_attentions=True, return_dict=True)
        
        # Attention map ì¶”ì¶œ
        attns = outputs.attentions
        if not attns or len(attns) == 0:
            time_attention = torch.ones(T_out) * 0.5
            freq_attention = torch.ones(F_out) * 0.5
        else:
            A = attns[-1]
            cls_to_patches = A[0, :, 0, 2:].mean(dim=0)
            
            Fp, Tp = 12, 101
            expected_len = Fp * Tp
            
            if cls_to_patches.numel() != expected_len:
                actual_len = cls_to_patches.numel()
                if actual_len < expected_len:
                    cls_to_patches = F.pad(cls_to_patches, (0, expected_len - actual_len))
                else:
                    cls_to_patches = cls_to_patches[:expected_len]
            
            # 2D ë§µìœ¼ë¡œ ì¬êµ¬ì„±
            full_map = cls_to_patches.reshape(Fp, Tp)  # [12, 101]
            
            # ì‹œê°„ ì–´í…ì…˜ (ì£¼íŒŒìˆ˜ ì°¨ì›ìœ¼ë¡œ í‰ê· )
            time_attn = full_map.mean(dim=0)  # [101]
            time_attn_interp = F.interpolate(time_attn.view(1,1,-1), size=T_out, mode="linear", align_corners=False).view(-1)
            time_attention = norm01(smooth1d(time_attn_interp, SMOOTH_T))
            
            # ì£¼íŒŒìˆ˜ ì–´í…ì…˜ (ì‹œê°„ ì°¨ì›ìœ¼ë¡œ í‰ê· )
            freq_attn = full_map.mean(dim=1)  # [12]
            freq_attn_interp = F.interpolate(freq_attn.view(1,1,-1), size=F_out, mode="linear", align_corners=False).view(-1)
            freq_attention = norm01(freq_attn_interp)
        
        # CLS features ì¶”ì¶œ
        if hasattr(outputs, 'last_hidden_state'):
            cls_features = outputs.last_hidden_state[:, 0, :]  # CLS token features
        else:
            cls_features = outputs.logits
        
        # ìºì‹±
        self.attention_cache[cache_key] = time_attention
        self.freq_attention_cache[cache_key] = freq_attention
        self.cls_head_cache[cache_key] = cls_features
        self.spectrogram_cache[cache_key] = mel_spec.clone()
        
        return time_attention, freq_attention, cls_features
    
    def _stft_all(self, audio: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """STFT ë³€í™˜ ë° Mel ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±"""
        # int16 ë°ì´í„°ë¥¼ float32ë¡œ ë³€í™˜ (STFT ìš”êµ¬ì‚¬í•­)
        if audio.dtype == np.int16:
            audio = audio.astype(np.float32) / 32767.0
        elif audio.dtype != np.float32:
            audio = audio.astype(np.float32)
        
        wav = torch.from_numpy(audio)
        st = torch.stft(wav, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN,
                        window=WINDOW, return_complex=True, center=True)
        mag = st.abs()
        P = (mag * mag).clamp_min(EPS)
        phase = torch.angle(st)

        if self.mel_fb_m2f.shape[0] != N_MELS:
            mel_fb_m2f = self.mel_fb_m2f.T.contiguous()
        else:
            mel_fb_m2f = self.mel_fb_m2f
        assert mel_fb_m2f.shape[0] == N_MELS and mel_fb_m2f.shape[1] == P.shape[0]
        mel_fb_m2f = mel_fb_m2f.to(P.dtype).to(P.device)
        mel_pow = (mel_fb_m2f @ P).clamp_min(EPS)
        return st, mag, P, phase, mel_pow
    
    def _purity_from_P(self, P: torch.Tensor) -> torch.Tensor:
        """ìˆœìˆ˜ë„ ê³„ì‚°"""
        fbins, T = P.shape
        e = P.sum(dim=0); e_n = e / (e.max() + EPS)
        p = P / (P.sum(dim=0, keepdim=True) + EPS)
        H = -(p * (p + EPS).log()).sum(dim=0)
        Hn = H / np.log(max(2, fbins))
        pur = W_E * e_n + (1.0 - W_E) * (1.0 - Hn)
        return norm01(smooth1d(pur, SMOOTH_T))
    
    def _anchor_score(self, A_t: torch.Tensor, Pur: torch.Tensor) -> torch.Tensor:
        """ì•µì»¤ ìŠ¤ì½”ì–´ ê³„ì‚°"""
        return norm01(smooth1d((A_t.clamp(0,1)**ALPHA_ATT) * (Pur.clamp(0,1)**BETA_PUR), SMOOTH_T))
    
    def _pick_anchor_region(self, score: torch.Tensor, La: int, core_pct: float, P: torch.Tensor) -> Tuple[int, int, int, int]:
        """ì•µì»¤ ì˜ì—­ ì„ íƒ"""
        T = score.numel()
        
        # ì „ì²´ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì˜ ì—ë„ˆì§€ ê³„ì‚°
        total_energy = P.sum(dim=0)  # [T]
        energy_threshold = torch.quantile(total_energy, 0.1)  # í•˜ìœ„ 10% ì—ë„ˆì§€ ì„ê³„ê°’
        
        # ì—ë„ˆì§€ê°€ ë„ˆë¬´ ë‚®ì€ êµ¬ê°„ì€ ì•µì»¤ í›„ë³´ì—ì„œ ì œì™¸
        valid_regions = total_energy > energy_threshold
        
        # ìœ íš¨í•œ êµ¬ê°„ì—ì„œë§Œ ì•µì»¤ ì„ íƒ
        if valid_regions.sum() == 0:
            peak_idx = int(torch.argmax(score).item())
        else:
            valid_score = score.clone()
            valid_score[~valid_regions] = -float('inf')
            peak_idx = int(torch.argmax(valid_score).item())
        
        anchor_s = max(0, min(peak_idx - (La // 2), T - La))
        anchor_e = anchor_s + La
        local_score = score[anchor_s:anchor_e]
        peak_idx_rel = int(torch.argmax(local_score).item())
        threshold = torch.quantile(local_score, core_pct)
        
        core_s_rel = peak_idx_rel
        while core_s_rel > 0 and local_score[core_s_rel - 1] >= threshold:
            core_s_rel -= 1
            
        core_e_rel = peak_idx_rel
        while core_e_rel < La - 1 and local_score[core_e_rel + 1] >= threshold:
            core_e_rel += 1
        
        core_e_rel += 1
        return anchor_s, anchor_e, core_s_rel, core_e_rel
    
    def _omega_support_with_ast_freq(self, Ablk: torch.Tensor, ast_freq_attn: torch.Tensor, strategy: str = "conservative") -> torch.Tensor:
        """ì£¼íŒŒìˆ˜ ì§€ì› ì˜ì—­ ê³„ì‚°"""
        if strategy == "conservative":
            omega_q = OMEGA_Q_CONSERVATIVE
            ast_freq_quantile = AST_FREQ_QUANTILE_CONSERVATIVE
        else:
            omega_q = OMEGA_Q_AGGRESSIVE
            ast_freq_quantile = AST_FREQ_QUANTILE_AGGRESSIVE
        
        med = Ablk.median(dim=1).values
        th = torch.quantile(med, omega_q)
        mask_energy = (med >= th).float()
        
        ast_freq_th = torch.quantile(ast_freq_attn, ast_freq_quantile)
        mask_ast_freq = (ast_freq_attn >= ast_freq_th).float()
        
        mask = torch.maximum(mask_energy, mask_ast_freq)
        
        for _ in range(OMEGA_DIL):
            mask = torch.maximum(mask, torch.roll(mask, 1))
            mask = torch.maximum(mask, torch.roll(mask, -1))
        
        if int(mask.sum().item()) < OMEGA_MIN_BINS:
            order = torch.argsort(med, descending=True)
            need = OMEGA_MIN_BINS - int(mask.sum().item())
            take = order[:need]
            mask[take] = 1.0
        
        return mask
    
    def _template_from_anchor_block(self, Ablk: torch.Tensor, omega: torch.Tensor) -> torch.Tensor:
        """ì•µì»¤ ë¸”ë¡ì—ì„œ í…œí”Œë¦¿ ìƒì„±"""
        om = omega.view(-1,1)
        w = (Ablk * om).mean(dim=1) * omega
        w = w / (w.sum() + EPS)
        w_sm = F.avg_pool1d(w.view(1,1,-1), kernel_size=3, stride=1, padding=1).view(-1)
        w = (w_sm * omega); w = w / (w.sum() + EPS)
        return w
    
    def _presence_from_energy(self, Xmel: torch.Tensor, omega: torch.Tensor) -> torch.Tensor:
        """ì—ë„ˆì§€ ê¸°ë°˜ ì¡´ì¬ê° ê³„ì‚°"""
        om = omega.view(-1,1)
        e_omega = (Xmel * om).sum(dim=0)
        e_omega = smooth1d(e_omega, PRES_SMOOTH_T)
        thr = torch.quantile(e_omega, PRES_Q)
        thr = torch.clamp(thr, min=1e-10)
        return (e_omega > thr).float()
    
    def _amplitude_raw(self, Xmel: torch.Tensor, w_bar: torch.Tensor, omega: torch.Tensor) -> torch.Tensor:
        """ì›ì‹œ ì§„í­ ê³„ì‚°"""
        om = omega.view(-1,1)
        Xo = Xmel * om
        denom = (w_bar*w_bar).sum() + EPS
        a_raw = (w_bar.view(1,-1) @ Xo).view(-1) / denom
        return a_raw.clamp_min(0.0)
    
    def _cos_similarity_over_omega(self, Xmel: torch.Tensor, w_bar: torch.Tensor, omega: torch.Tensor, g_pres: torch.Tensor):
        """ì˜¤ë©”ê°€ ì˜ì—­ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        om = omega.view(-1,1)
        Xo = Xmel * om
        wn = (w_bar * omega); wn = wn / (wn.norm(p=2) + 1e-8)
        Xn = Xo / (Xo.norm(p=2, dim=0, keepdim=True) + 1e-8)
        cos_raw = (wn.view(-1,1) * Xn).sum(dim=0).clamp(0,1)
        return cos_raw * g_pres
    
    def _unified_masking_strategy(self, Xmel: torch.Tensor, w_bar: torch.Tensor, omega: torch.Tensor, 
                                 ast_freq_attn: torch.Tensor, P: torch.Tensor, s: int, e: int, strategy: str = "conservative") -> torch.Tensor:
        """í†µí•© ë§ˆìŠ¤í‚¹ ì „ëµ"""
        fbins, T = P.shape
        
        # Calculate cosÎ©, the core of our mask
        g_pres = self._presence_from_energy(Xmel, omega)
        cos_t_raw = self._cos_similarity_over_omega(Xmel, w_bar, omega, g_pres)

        # Map Î©(mel)->Î©(linear) for frequency weighting
        if omega.shape[0] == self.mel_fb_m2f.shape[0]:
            omega_lin = ((self.mel_fb_m2f @ omega).clamp_min(0.0) > 1e-12).float()
        else:
            omega_lin = torch.ones(self.mel_fb_m2f.shape[1], device=omega.device)
        
        # Enhanced Masking Logic
        anchor_spec = P[:, s:e]
        anchor_energy = anchor_spec.sum().item()
        total_energy = P.sum().item()
        energy_ratio = anchor_energy / (total_energy + 1e-8)
        
        is_weak_sound = energy_ratio < 0.1
        
        # ê¸°ë³¸ ë§ˆìŠ¤í¬: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì œê³±ìœ¼ë¡œ ì•½í™”
        cos_squared = cos_t_raw ** 2
        
        # Threshold ê¸°ë°˜ ë§ˆìŠ¤í‚¹ (sigmoid ëŒ€ì‹ )
        base_threshold = THRESHOLD_PRESETS.get(CURRENT_THRESHOLD_PRESET, MASK_THRESHOLD)
        
        if is_weak_sound:
            # ì•½í•œ ì†Œë¦¬ì˜ ê²½ìš° ë” ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš©
            threshold = base_threshold * 0.8
            softness = MASK_SOFTNESS * 1.2
        else:
            # ì¼ë°˜ ì†Œë¦¬ì˜ ê²½ìš° ê¸°ë³¸ ì„ê³„ê°’ ì‚¬ìš©
            threshold = base_threshold
            softness = MASK_SOFTNESS
        
        # Threshold ê¸°ë°˜ ë§ˆìŠ¤í‚¹
        if USE_HARD_THRESHOLD:
            # ì™„ì „í•œ í•˜ë“œ threshold (0 ë˜ëŠ” 1)
            soft_time_mask = (cos_squared >= threshold).float()
            mask_type = "HARD"
        else:
            # ë¶€ë“œëŸ¬ìš´ threshold ë§ˆìŠ¤í‚¹
            # threshold Â± softness ë²”ìœ„ì—ì„œ ì„ í˜• ë³´ê°„
            mask_input = (cos_squared - (threshold - softness)) / (2 * softness)
            soft_time_mask = torch.clamp(mask_input, 0.0, 1.0)
            mask_type = "SOFT"
        
        # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        mask_mean = soft_time_mask.mean().item()
        mask_std = soft_time_mask.std().item()
        cos_squared_mean = cos_squared.mean().item()
        print(f"[Separator] {mask_type} threshold masking ({CURRENT_THRESHOLD_PRESET}) - Threshold: {threshold:.3f}, CosÂ² mean: {cos_squared_mean:.3f}, Mask mean: {mask_mean:.3f}Â±{mask_std:.3f}")
        
        # ì•µì»¤ ì˜ì—­ì˜ ì§„í­ ì£¼íŒŒìˆ˜ ì„ íƒ
        anchor_max_amp = anchor_spec.max(dim=1).values
        
        if is_weak_sound:
            amp_threshold = torch.quantile(anchor_max_amp, 0.6)
        else:
            amp_threshold = torch.quantile(anchor_max_amp, 0.7)
        
        high_amp_mask_lin = (anchor_max_amp >= amp_threshold).float()
        
        # ì•µì»¤ ì˜ì—­ì—ì„œ í™œì„±í™”ëœ AST ì£¼íŒŒìˆ˜ ì„ íƒ
        anchor_ast_freq = ast_freq_attn.clone()
        
        if is_weak_sound:
            ast_freq_threshold = torch.quantile(anchor_ast_freq, 0.5)
        else:
            ast_freq_threshold = torch.quantile(anchor_ast_freq, 0.4)
        
        ast_active_mask_mel = (anchor_ast_freq >= ast_freq_threshold).float()
        
        # AST ì£¼íŒŒìˆ˜ ë§ˆìŠ¤í¬ë¥¼ Melì—ì„œ Linear ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜
        if ast_active_mask_mel.shape[0] == self.mel_fb_m2f.shape[0]:
            ast_active_mask_lin = ((self.mel_fb_m2f @ ast_active_mask_mel).clamp_min(0.0) > 0.2).float()
        else:
            ast_active_mask_lin = torch.ones(self.mel_fb_m2f.shape[1], device=ast_freq_attn.device)
        
        # ì„ íƒëœ ì£¼íŒŒìˆ˜ ì˜ì—­ ê²°í•©
        if high_amp_mask_lin.shape[0] != ast_active_mask_lin.shape[0]:
            min_size = min(high_amp_mask_lin.shape[0], ast_active_mask_lin.shape[0])
            high_amp_mask_lin = high_amp_mask_lin[:min_size]
            ast_active_mask_lin = ast_active_mask_lin[:min_size]
        
        freq_boost_mask = torch.maximum(high_amp_mask_lin, ast_active_mask_lin)
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        if is_weak_sound:
            freq_weight = 1.0 + 0.8 * freq_boost_mask
        else:
            freq_weight = 1.0 + 0.6 * freq_boost_mask
        
        # ê¸°ë³¸ ë§ˆìŠ¤í¬ ê³„ì‚°
        M_base = omega_lin.view(-1, 1) * soft_time_mask.view(1, -1)
        
        # ì£¼íŒŒìˆ˜ ê°€ì¤‘ì¹˜ ì ìš©
        M_weighted = M_base * freq_weight.view(-1, 1)
        
        # ë§ˆìŠ¤í¬ê°€ ì‹¤ì œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ë³´ë‹¤ í¬ì§€ ì•Šë„ë¡ ì œí•œ
        spec_magnitude = P.sqrt()
        
        if M_weighted.shape[0] != spec_magnitude.shape[0]:
            min_freq = min(M_weighted.shape[0], spec_magnitude.shape[0])
            M_weighted = M_weighted[:min_freq, :]
            spec_magnitude = spec_magnitude[:min_freq, :]
        
        M_lin = torch.minimum(M_weighted, spec_magnitude)
        
        # ë§ˆìŠ¤í¬ ê°•ë„ ì¡°ì •
        if is_weak_sound:
            M_lin = torch.minimum(M_lin, spec_magnitude * 0.95)
        else:
            M_lin = torch.minimum(M_lin, spec_magnitude * 0.8)
        
        return M_lin
    
    def _single_pass_separation(self, audio: np.ndarray, used_mask_prev: Optional[torch.Tensor],
                               prev_anchors: List[Tuple[float,float,torch.Tensor,torch.Tensor]],
                               pass_idx: int) -> Tuple[np.ndarray, np.ndarray, float, Optional[torch.Tensor], Dict[str, Any]]:
        """ë‹¨ì¼ íŒ¨ìŠ¤ ë¶„ë¦¬"""
        t0 = time.time()
        st, mag, P, phase, Xmel = self._stft_all(audio)
        fbins, T = P.shape
        La = int(round(ANCHOR_SEC * SR / HOP))

        # ìºì‹±ëœ attention map ì‚¬ìš©
        time_attention, ast_freq_attn, _ = self._extract_and_cache_attention(audio, T, N_MELS)
        
        A_t = time_attention
        Pur = self._purity_from_P(P)
        Sc = self._anchor_score(A_t, Pur)

        # Suppress used frames
        if used_mask_prev is not None:
            um = align_len_1d(used_mask_prev, T, device=Sc.device, mode="linear")
            k = int(round((USED_DILATE_MS/1000.0)*SR/HOP)); k = ensure_odd(max(1,k))
            ker = torch.ones(k, device=Sc.device)/k
            um = (F.conv1d(um.view(1,1,-1), ker.view(1,1,-1), padding=k//2).view(-1) > 0.2).float()
            Sc = Sc * (1 - 0.85 * um)

        # Enhanced suppression of previous anchors
        for (sa, ea, prev_w, prev_omega) in prev_anchors:
            ca = int(((sa+ea)/2) * SR / HOP)
            ca = max(0, min(T-1, ca))
            sigma = int(round((ANCHOR_SUPPRESS_MS/1000.0)*SR/HOP))
            idx = torch.arange(T, device=Sc.device) - ca
            Sc = Sc * (1 - ANCHOR_SUPPRESS_BASE * torch.exp(-(idx**2)/(2*(sigma**2)+1e-8)))
            core_s = max(0, ca - La//2); core_e = min(T, ca + La//2)
            Sc[core_s:core_e] *= 0.2
        
        # Pick anchor and core regions
        s, e, core_s_rel, core_e_rel = self._pick_anchor_region(Sc, La, TOP_PCT_CORE_IN_ANCHOR, P)
        
        # Create anchor block
        Ablk = Xmel[:, s:e].clone()
        if core_s_rel > 0:  Ablk[:, :core_s_rel] = 0
        if core_e_rel < La: Ablk[:, core_e_rel:] = 0

        # Î© ê³„ì‚°
        omega = self._omega_support_with_ast_freq(Ablk, ast_freq_attn, "conservative")
        w_bar = self._template_from_anchor_block(Ablk, omega)
        
        # í†µí•© ë§ˆìŠ¤í‚¹ ì „ëµ ì ìš©
        M_lin = self._unified_masking_strategy(Xmel, w_bar, omega, ast_freq_attn, P, s, e, "conservative")
        
        # Subtraction in the complex STFT domain
        stft_full = st
        
        # ë§ˆìŠ¤í¬ ì ìš©
        if M_lin.shape[0] != mag.shape[0]:
            min_freq = min(M_lin.shape[0], mag.shape[0])
            M_lin = M_lin[:min_freq, :]
            mag = mag[:min_freq, :]
            phase = phase[:min_freq, :]
        
        mag_linear = mag
        mag_masked_linear = M_lin * mag_linear
        
        stft_src = mag_masked_linear * torch.exp(1j * phase)
        
        # ì”ì—¬ë¬¼ ê³„ì‚°
        mag_residual_linear = mag_linear - mag_masked_linear
        mag_residual_linear = torch.maximum(mag_residual_linear, torch.zeros_like(mag_residual_linear))
        stft_res = mag_residual_linear * torch.exp(1j * phase)
        
        # ì—ë„ˆì§€ ê²€ì¦
        src_energy = torch.sum(torch.abs(stft_src)**2).item()
        res_energy = torch.sum(torch.abs(stft_res)**2).item()
        orig_energy = torch.sum(torch.abs(stft_full)**2).item()
        total_energy = src_energy + res_energy
        
        # ì—ë„ˆì§€ ë³´ì¡´ ê²€ì¦ ë° ì •ê·œí™”
        energy_ratio = total_energy / (orig_energy + 1e-8)
        if energy_ratio > 1.05:
            scale_factor = orig_energy / (total_energy + 1e-8)
            scale_tensor = torch.tensor(scale_factor, device=stft_src.device, dtype=stft_src.dtype)
            stft_src = stft_src * torch.sqrt(scale_tensor)
            stft_res = stft_res * torch.sqrt(scale_tensor)
        elif energy_ratio < 0.95:
            scale_factor = orig_energy / (total_energy + 1e-8)
            scale_tensor = torch.tensor(scale_factor, device=stft_src.device, dtype=stft_src.dtype)
            stft_src = stft_src * torch.sqrt(scale_tensor)
            stft_res = stft_res * torch.sqrt(scale_tensor)

        # Reconstruct both source and residual
        if stft_src.shape[0] != N_FFT//2 + 1:
            target_freq = N_FFT//2 + 1
            if stft_src.shape[0] < target_freq:
                pad_size = target_freq - stft_src.shape[0]
                stft_src = F.pad(stft_src, (0, 0, 0, pad_size), mode='constant', value=0)
                stft_res = F.pad(stft_res, (0, 0, 0, pad_size), mode='constant', value=0)
            else:
                stft_src = stft_src[:target_freq, :]
                stft_res = stft_res[:target_freq, :]
        
        src_amp = torch.istft(stft_src, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN, 
                             window=WINDOW, center=True, length=L_FIXED).detach().cpu().numpy()
        res = torch.istft(stft_res, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN, 
                          window=WINDOW, center=True, length=L_FIXED).detach().cpu().numpy()

        # ER calculation
        e_src = float(np.sum(src_amp**2)); e_res = float(np.sum(res**2))
        er = e_src / (e_src + e_res + 1e-12)

        # ë¶„ë¥˜ ë° ìˆœìˆ˜ë„ ê³„ì‚°
        cache_key = self._get_cache_key(audio)
        cls_features = self.cls_head_cache.get(cache_key)
        
        if cls_features is not None:
            if cls_features.shape[-1] == self.ast_model.config.num_labels:
                logits = cls_features
            else:
                logits = self.ast_model.classifier(cls_features)
            
            probabilities = torch.softmax(logits, dim=-1)
            predicted_class_id = logits.argmax(dim=-1).item()
            confidence = probabilities[0, predicted_class_id].item()
            
            class_name = self.ast_model.config.id2label[predicted_class_id]
            sound_type = self._get_sound_type(predicted_class_id)
        else:
            class_name, sound_type, predicted_class_id, confidence = "Unknown", "other", 0, 0.0

        # Used-frame mask for next pass
        if M_lin.shape[0] != P.shape[0]:
            min_freq = min(M_lin.shape[0], P.shape[0])
            M_lin = M_lin[:min_freq, :]
            P = P[:min_freq, :]
        
        r_t = (M_lin * P).sum(dim=0) / (P.sum(dim=0) + 1e-8)
        used_mask = (r_t >= USED_THRESHOLD).float()

        elapsed = time.time() - t0
        
        info = {
            "er": er,
            "elapsed": elapsed,
            "anchor": (s*HOP/SR, e*HOP/SR),
            "core": ((s+core_s_rel)*HOP/SR, (s+core_e_rel)*HOP/SR),
            "quality": float(M_lin.mean().item()),
            "w_bar": w_bar,
            "omega": omega,
            "stopped": False,
            "energy_ratio": energy_ratio,
            "class_name": class_name,
                "sound_type": sound_type,
            "class_id": predicted_class_id,
            "confidence": confidence
        }
        
        return src_amp, res, er, used_mask, info
    
    def separate_audio(self, audio: np.ndarray, max_passes: int = MAX_PASSES, on_pass_complete=None) -> List[Dict[str, Any]]:
        """ì˜¤ë””ì˜¤ ë¶„ë¦¬ ì‹¤í–‰ (ê° íŒ¨ìŠ¤ë§ˆë‹¤ ì¦‰ì‹œ ì²˜ë¦¬)"""
        if not self.is_available:
            print("[Separator] âŒ Model not available for separation")
            return []
        
        print(f"[Separator] Starting audio separation with {max_passes} passes...")
        
        current_audio = audio.copy()
        used_mask_prev = None
        prev_anchors = []
        sources = []
        
        for pass_idx in range(max_passes):
            print(f"[Separator] --- Pass {pass_idx + 1} ---")
            
            # ë¶„ë¦¬ ì‹¤í–‰
            src_amp, res, er, used_mask, info = self._single_pass_separation(
                current_audio, used_mask_prev, prev_anchors, pass_idx
            )
            
            # ë¶„ë¦¬ ê²°ê³¼ ë””ë²„ê¹…
            if src_amp is not None:
                src_rms = np.sqrt(np.mean(src_amp ** 2))
                res_rms = np.sqrt(np.mean(res ** 2))
                print(f"[Separator] Pass {pass_idx + 1} - Source RMS: {src_rms:.6f}, Residual RMS: {res_rms:.6f}, Energy ratio: {er:.6f}")
            else:
                print(f"[Separator] Pass {pass_idx + 1} - Source is None!")
            
            # ê²°ê³¼ ì €ì¥
            source_info = {
                "pass": pass_idx + 1,
                "class_name": info['class_name'],
                "sound_type": info['sound_type'],
                "confidence": info['confidence'],
                "energy_ratio": er,
                "anchor": info['anchor'],
                "audio": src_amp
            }
            sources.append(source_info)
            
            # ê° íŒ¨ìŠ¤ ì™„ë£Œ ì¦‰ì‹œ ì²˜ë¦¬ (ë°±ì—”ë“œ ì „ì†¡, LED ì¶œë ¥)
            # ì¶œë ¥ ê°„ì†Œí™” - ì½œë°±ì—ì„œ ì²˜ë¦¬
            
            # ì½œë°± í•¨ìˆ˜ í˜¸ì¶œ (ê° íŒ¨ìŠ¤ ì™„ë£Œ ì‹œë§ˆë‹¤)
            if on_pass_complete:
                on_pass_complete(source_info)
            
            # ì”ì—¬ë¬¼ì„ ë‹¤ìŒ íŒ¨ìŠ¤ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© (ì¦í­ ì ìš©)
            if RESIDUAL_AMPLIFY and pass_idx < max_passes - 1:  # ë§ˆì§€ë§‰ íŒ¨ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                # ì”ì—¬ë¬¼ ì¦í­
                current_audio = amplify_residual(res, RESIDUAL_GAIN, RESIDUAL_MAX_GAIN)
                print(f"[Separator] Residual amplified by {RESIDUAL_GAIN}x for next pass")
            else:
                current_audio = res
            
            used_mask_prev = used_mask
            
            # ì•µì»¤ ì •ë³´ ì €ì¥
            prev_anchors.append((info['anchor'][0], info['anchor'][1], info['w_bar'], info['omega']))
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            if er < MIN_ERATIO:
                print(f"[Separator] Early stop: Energy ratio {er:.3f} < {MIN_ERATIO}")
                break
        
        print(f"[Separator] Separation completed. Found {len(sources)} sources.")
        return sources
    
    def is_model_available(self) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self.is_available
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        # ìºì‹œ ì •ë¦¬
        self.attention_cache.clear()
        self.freq_attention_cache.clear()
        self.cls_head_cache.clear()
        self.spectrogram_cache.clear()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()




def create_sound_separator(model_name: str = "MIT/ast-finetuned-audioset-10-10-0.4593", 
                          device: str = "auto", backend_url: str = BACKEND_URL) -> SoundSeparator:
    """
    Sound Separator ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì‹¤ì „ìš©)
    
    Args:
        model_name: AST ëª¨ë¸ ì´ë¦„
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        backend_url: ë°±ì—”ë“œ API URL
        
    Returns:
        SoundSeparator ì¸ìŠ¤í„´ìŠ¤
    """
    return SoundSeparator(model_name, device, backend_url)


def main():
    """ì‹¤ì „ìš© ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸµ Sound Separator - ì‹¤ì „ ëª¨ë“œ")
    print("=" * 50)
    print("ì´ ëª¨ë“ˆì€ sound_pipeline.pyì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
    print("ì§ì ‘ ì‹¤í–‰í•˜ì§€ ë§ˆì„¸ìš”.")
    print("=" * 50)


if __name__ == "__main__":
    main()
