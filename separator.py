#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AST-guided Source Separator
A unified pipeline for audio source separation using AST (Audio Spectrogram Transformer) model:
- Enhanced Frequency Attention with AST model integration
- Adaptive masking strategy with conservative and aggressive modes
- Energy conservation with fallback mechanisms
- Comprehensive classification and analysis
- Multi-pass separation with intelligent anchor selection
"""

import os, time, warnings, argparse
from typing import List, Tuple, Optional
import numpy as np
import torch
import torch.nn.functional as F
import torchaudio
import matplotlib.pyplot as plt
from transformers import ASTFeatureExtractor, ASTForAudioClassification

warnings.filterwarnings("ignore")
torch.set_num_threads(4)

# =========================
# Config


# =========================
SR                = 16000  # ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Hz) - AST ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ” 16kHz
WIN_SEC           = 4.096  # ë¶„ì„ ìœˆë„ìš° ê¸¸ì´ (ì´ˆ) - 4ì´ˆ ê³ ì • ìœˆë„ìš°ë¡œ ì²˜ë¦¬
ANCHOR_SEC        = 0.512  # ì•µì»¤ êµ¬ê°„ ê¸¸ì´ (ì´ˆ) - 512ms ì•µì»¤ë¡œ ì†ŒìŠ¤ íŠ¹ì„± ì¶”ì¶œ
L_FIXED           = int(round(WIN_SEC * SR))  # ê³ ì • ì˜¤ë””ì˜¤ ê¸¸ì´ (ìƒ˜í”Œ ìˆ˜)

# === Final Output Processing ===
NORMALIZE_TARGET_PEAK = 0.95  # ìµœëŒ€ ë³¼ë¥¨ì˜ 95% í¬ê¸°ë¡œ í‘œì¤€í™” (í´ë¦¬í•‘ ë°©ì§€)
RESIDUAL_CLIP_THR = 0.0005    # ìµœì¢… ì”ì—¬ë¬¼ì˜ ì§„í­ì´ ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ë§Œë“¦ (ë…¸ì´ì¦ˆ ì œê±°)

# === Adaptive Masking Strategy ===
USE_ADAPTIVE_STRATEGY = True   # ì ì‘ì  ë§ˆìŠ¤í‚¹ ì „ëµ ì‚¬ìš© ì—¬ë¶€
FALLBACK_THRESHOLD = 0.1       # ì—ë„ˆì§€ ë³´ì¡´ ì‹¤íŒ¨ ì‹œ fallback ì „ëµ ì‚¬ìš© ì„ê³„ê°’

# === Soft Masking Parameters ===
MASK_SIGMOID_CENTER = 0.6   # ë§ˆìŠ¤í¬ê°€ 0.5ê°€ ë˜ëŠ” cosÎ© ê°’ (ì¤‘ì‹¬ì ) - ë‚®ì„ìˆ˜ë¡ ë” ê°•í•œ ë§ˆìŠ¤í¬
MASK_SIGMOID_SLOPE  = 20.0  # S-ì»¤ë¸Œì˜ ê²½ì‚¬ - ë†’ì„ìˆ˜ë¡ í•˜ë“œ ë§ˆìŠ¤í¬ì²˜ëŸ¼ ë‚ ì¹´ë¡œì›Œì§

# STFT Parameters (10ms hop)
N_FFT, HOP, WINLEN = 400, 160, 400  # FFT í¬ê¸°, í™‰ ê¸¸ì´, ìœˆë„ìš° ê¸¸ì´ (10ms í™‰ìœ¼ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬)
WINDOW = torch.hann_window(WINLEN)  # Hann ìœˆë„ìš° í•¨ìˆ˜
EPS = 1e-10  # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’

# Mel Scale Parameters
N_MELS = 128  # Mel ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë¹ˆ ìˆ˜ - AST ëª¨ë¸ê³¼ í˜¸í™˜

# Anchor Score Parameters
SMOOTH_T      = 19           # ì‹œê°„ì¶• ìŠ¤ë¬´ë”© ì»¤ë„ í¬ê¸° (í™‰ ë‹¨ìœ„)
ALPHA_ATT     = 0.80         # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì§€ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì–´í…ì…˜ ì˜í–¥ ê°ì†Œ)
BETA_PUR      = 1.20         # ìˆœë„(purity) ê°€ì¤‘ì¹˜ ì§€ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìˆœë„ ì¤‘ì‹œ)
W_E           = 0.30         # ì—ë„ˆì§€ì™€ ì—”íŠ¸ë¡œí”¼ì˜ ê°€ì¤‘ì¹˜ ë¹„ìœ¨
TOP_PCT_CORE_IN_ANCHOR  = 0.50  # ì•µì»¤ ë‚´ ì½”ì–´ ì˜ì—­ ë¹„ìœ¨ (50%)

# Î© & Template Parameters (Adaptive)
OMEGA_Q_CONSERVATIVE = 0.2   # ë³´ìˆ˜ì  ì „ëµìš©
OMEGA_Q_AGGRESSIVE   = 0.7   # ê³µê²©ì  ì „ëµìš©
OMEGA_DIL         = 2        # Î© ë§ˆìŠ¤í¬ íŒ½ì°½ ë°˜ë³µ íšŸìˆ˜ (ì¸ì ‘ ì£¼íŒŒìˆ˜ í¬í•¨)
OMEGA_MIN_BINS    = 5        # ìµœì†Œ ì„ íƒí•  ì£¼íŒŒìˆ˜ ë¹ˆ ìˆ˜ (ë„ˆë¬´ ì ì€ ì„ íƒ ë°©ì§€)

# AST Frequency Attention Parameters (Adaptive)
AST_FREQ_QUANTILE_CONSERVATIVE = 0.4  # ë³´ìˆ˜ì  ì „ëµìš©
AST_FREQ_QUANTILE_AGGRESSIVE   = 0.2  # ê³µê²©ì  ì „ëµìš©

# Sound Type Classification
DANGER_IDS = {396, 397, 398, 399, 400, 426, 436}
HELP_IDS = {23, 14, 354, 355, 356, 359}
WARNING_IDS = {288, 364, 388, 389, 390, 439, 391, 392, 393, 395, 440, 441, 443, 456, 469, 470, 478, 479}

# Presence Gate Parameters
PRES_Q            = 0.20     # Presence gate quantile (ìƒìœ„ 80% ì—ë„ˆì§€ êµ¬ê°„ì—ì„œ presence íŒë‹¨)
PRES_SMOOTH_T     = 9        # Presence gate ì‹œê°„ì¶• ìŠ¤ë¬´ë”© ì»¤ë„ í¬ê¸°

# Suppression Parameters (ì´ì „ íŒ¨ìŠ¤ ì–µì œ)
USED_THRESHOLD        = 0.65  # ì‚¬ìš©ëœ í”„ë ˆì„ íŒë‹¨ ì„ê³„ê°’ (65% ì´ìƒ ë§ˆìŠ¤í¬ëœ í”„ë ˆì„)
USED_DILATE_MS        = 80    # ì‚¬ìš©ëœ í”„ë ˆì„ ì£¼ë³€ í™•ì¥ ì‹œê°„ (ms)
ANCHOR_SUPPRESS_MS    = 200   # ì´ì „ ì•µì»¤ ì¤‘ì‹¬ ì–µì œ ë°˜ê²½ (ms)
ANCHOR_SUPPRESS_BASE  = 0.6   # ì´ì „ ì•µì»¤ ì–µì œ ê°•ë„ (60% ì–µì œ)

# Loop Control Parameters
MAX_PASSES    = 3      # ìµœëŒ€ ë¶„ë¦¬ íŒ¨ìŠ¤ ìˆ˜ (3ë²ˆê¹Œì§€ ë°˜ë³µ)
MIN_ERATIO    = 0.01   # ìµœì†Œ ì—ë„ˆì§€ ë¹„ìœ¨ (1% ë¯¸ë§Œì´ë©´ ì¤‘ë‹¨)

# =========================
# Utils
# =========================
def norm01(x: torch.Tensor) -> torch.Tensor:
    return (x - x.min()) / (x.max() - x.min() + 1e-8)

def ensure_odd(k: int) -> int:
    return k + 1 if (k % 2 == 0) else k

def smooth1d(x: torch.Tensor, k: int) -> torch.Tensor:
    if k <= 1: return x
    ker = torch.ones(k, device=x.device) / k
    return F.conv1d(x.view(1,1,-1), ker.view(1,1,-1), padding=k//2).view(-1)

def to_np(x: torch.Tensor) -> np.ndarray:
    return x.detach().cpu().numpy()

def align_len_1d(x: torch.Tensor, T: int, device=None, mode="linear"):
    if device is None: device = x.device
    xv = x.to(device).view(1,1,-1).float()
    if xv.size(-1) == T:
        out = xv.view(-1)
    else:
        out = F.interpolate(xv, size=T, mode=mode, align_corners=False).view(-1)
    return out.clamp(0,1)

def soft_sigmoid(x: torch.Tensor, center: float, slope: float, min_val: float = 0.0) -> torch.Tensor:
    sig = torch.sigmoid(slope * (x - center))
    return min_val + (1.0 - min_val) * sig

def get_sound_type(class_id: int) -> str:
    """í´ë˜ìŠ¤ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì†Œë¦¬ íƒ€ì… ë°˜í™˜"""
    if class_id in DANGER_IDS:
        return "danger"
    elif class_id in HELP_IDS:
        return "help"
    elif class_id in WARNING_IDS:
        return "warning"
    else:
        return "other"

def calculate_decibel(audio: np.ndarray) -> Tuple[float, float, float]:
    """ì˜¤ë””ì˜¤ì˜ ë°ì‹œë²¨ ê³„ì‚° (min, max, í‰ê· )"""
    # RMS ê³„ì‚°
    rms = np.sqrt(np.mean(audio**2))
    if rms == 0:
        return -np.inf, -np.inf, -np.inf
    
    # ë°ì‹œë²¨ ë³€í™˜ (20 * log10(rms))
    db = 20 * np.log10(rms + 1e-10)
    
    # min, max, í‰ê·  ê³„ì‚°
    db_min = 20 * np.log10(np.min(np.abs(audio)) + 1e-10)
    db_max = 20 * np.log10(np.max(np.abs(audio)) + 1e-10)
    db_mean = db
    
    return db_min, db_max, db_mean

def calculate_global_purity(Xmel: torch.Tensor, w_bar: torch.Tensor, omega: torch.Tensor) -> float:
    """ì „ì²´ ì˜¤ë””ì˜¤ì— ëŒ€í•œ ìˆœìˆ˜ë„ ê³„ì‚°"""
    g_pres = presence_from_energy(Xmel, omega)
    cos_t_raw = cos_similarity_over_omega(Xmel, w_bar, omega, g_pres)
    global_purity = cos_t_raw.mean().item()
    return global_purity

def should_skip_separation(confidence: float, purity: float, class_id: int) -> bool:
    """ë¶„ë¦¬ë¥¼ ê±´ë„ˆë›¸ì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜"""
    # ì‹ ë¢°ë„ ì„ê³„ê°’ (0.8 ì´ìƒ)
    confidence_threshold = 0.8
    
    # ìˆœìˆ˜ë„ ì„ê³„ê°’ (0.7 ì´ìƒ)
    purity_threshold = 0.7
    
    # "other" í´ë˜ìŠ¤ëŠ” ë¶„ë¦¬ ê±´ë„ˆë›°ì§€ ì•ŠìŒ
    if get_sound_type(class_id) == "other":
        return False
    
    # ì‹ ë¢°ë„ì™€ ìˆœìˆ˜ë„ê°€ ëª¨ë‘ ì„ê³„ê°’ ì´ìƒì´ë©´ ë¶„ë¦¬ ê±´ë„ˆë›°ê¸°
    return confidence >= confidence_threshold and purity >= purity_threshold

@torch.no_grad()
def classify_audio_segment(audio: np.ndarray, extractor, ast_model) -> Tuple[str, str, int, float]:
    """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¶„ë¥˜í•˜ì—¬ í´ë˜ìŠ¤ëª…, íƒ€ì…, ID, ì‹ ë¢°ë„ ë°˜í™˜ (ì”ì—¬ë¬¼ ë¶„ë¥˜ìš©)"""
    # 10ì´ˆë¡œ íŒ¨ë”©
    target_len = int(10.0 * SR)
    if len(audio) < target_len:
        audio_padded = np.zeros(target_len, dtype=np.float32)
        audio_padded[:len(audio)] = audio
    else:
        audio_padded = audio[:target_len]
    
    # AST ëª¨ë¸ë¡œ ë¶„ë¥˜
    feat = extractor(audio_padded, sampling_rate=SR, return_tensors="pt")
    outputs = ast_model(input_values=feat["input_values"])
    
    # Top-1 í´ë˜ìŠ¤ ì¶”ì¶œ ë° ì‹ ë¢°ë„ ê³„ì‚°
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=-1)
    predicted_class_id = logits.argmax(dim=-1).item()
    confidence = probabilities[0, predicted_class_id].item()
    
    class_name = ast_model.config.id2label[predicted_class_id]
    sound_type = get_sound_type(predicted_class_id)
    
    return class_name, sound_type, predicted_class_id, confidence

# =========================
# Debug Visualization
# =========================
def debug_plot(pass_idx: int, Sc: torch.Tensor, a_raw: torch.Tensor, cos_t_raw: torch.Tensor, 
               C_t: torch.Tensor, P: torch.Tensor, M_lin: torch.Tensor, full_map: torch.Tensor,
               s: int, e: int, core_s_rel: int, core_e_rel: int, ast_freq_attn: torch.Tensor,
               src_amp: np.ndarray, res: np.ndarray, png: str, title: str = "",
               original_audio: np.ndarray = None, global_confidence: float = 0.0, 
               global_purity: float = 0.0):
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    # ì œëª©ì— ì „ì²´ ì‹ ë¢°ë„ì™€ ìˆœìˆ˜ë„ ì¶”ê°€
    enhanced_title = f"{title}\nGlobal Confidence: {global_confidence:.3f} | Global Purity: {global_purity:.3f}"
    fig.suptitle(enhanced_title, fontsize=16, fontweight='bold')
    
    # === ì²« ë²ˆì§¸ í–‰: íŒŒí˜• (Waveforms) ===
    # 1. Original Audio Waveform
    ax = axes[0, 0]
    if original_audio is not None:
        time_axis = np.linspace(0, len(original_audio) / SR, len(original_audio))
        ax.plot(time_axis, original_audio, 'b-', alpha=0.7, linewidth=0.8)
        ax.set_title('Original Audio Waveform')
        ax.set_xlabel('Time (s)')
        ax.set_ylabel('Amplitude')
        ax.set_ylim(-1, 1)  # Amplitude ë²”ìœ„ í†µì¼
        ax.grid(True, alpha=0.3)
    else:
        ax.text(0.5, 0.5, 'No original audio', ha='center', va='center', transform=ax.transAxes)
        ax.set_title('Original Audio Waveform')
        ax.set_ylim(-1, 1)
    
    # 2. Separated Source Waveform
    ax = axes[0, 1]
    time_axis = np.linspace(0, len(src_amp) / SR, len(src_amp))
    ax.plot(time_axis, src_amp, 'g-', alpha=0.7, linewidth=0.8)
    ax.set_title('Separated Source Waveform')
    ax.set_xlabel('Time (s)')
    ax.set_ylabel('Amplitude')
    ax.set_ylim(-1, 1)  # Amplitude ë²”ìœ„ í†µì¼
    ax.grid(True, alpha=0.3)
    
    # 3. Residual Audio Waveform
    ax = axes[0, 2]
    time_axis = np.linspace(0, len(res) / SR, len(res))
    ax.plot(time_axis, res, 'r-', alpha=0.7, linewidth=0.8)
    ax.set_title('Residual Audio Waveform')
    ax.set_xlabel('Time (s)')
    ax.set_ylabel('Amplitude')
    ax.set_ylim(-1, 1)  # Amplitude ë²”ìœ„ í†µì¼
    ax.grid(True, alpha=0.3)
    
    # === ë‘ ë²ˆì§¸ í–‰: Mel ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë° ë¶„ì„ ===
    # 4. Anchor Score
    ax = axes[1, 0]
    T = Sc.numel()
    t_axis = np.arange(T) * HOP / SR
    ax.plot(t_axis, to_np(Sc), 'b-', linewidth=1.5, label='Anchor Score')
    ax.axvspan(s*HOP/SR, e*HOP/SR, alpha=0.3, color='red', label='Anchor Region')
    ax.axvspan((s+core_s_rel)*HOP/SR, (s+core_e_rel)*HOP/SR, alpha=0.5, color='orange', label='Core Region')
    ax.set_xlabel('Time (s)')
    ax.set_ylabel('Score')
    ax.set_title('Anchor Selection')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 5. Amplitude & Cosine Similarity
    ax = axes[1, 1]
    t_axis = np.arange(a_raw.numel()) * HOP / SR
    ax.plot(t_axis, to_np(a_raw), 'g-', linewidth=1.5, label='Amplitude')
    ax2 = ax.twinx()
    ax2.plot(t_axis, to_np(cos_t_raw), 'r-', linewidth=1.5, label='Cosine Similarity')
    ax.set_xlabel('Time (s)')
    ax.set_ylabel('Amplitude', color='g')
    ax2.set_ylabel('Cosine Similarity', color='r')
    ax.set_title('Amplitude & Cosine Similarity')
    ax.legend(loc='upper left')
    ax2.legend(loc='upper right')
    ax.grid(True, alpha=0.3)
    
    # 6. AST Frequency Attention
    ax = axes[1, 2]
    ax.plot(to_np(ast_freq_attn), 'purple', linewidth=2)
    ax.set_title('AST Frequency Attention')
    ax.set_xlabel('Mel bins')
    ax.set_ylabel('Attention weight')
    ax.grid(True, alpha=0.3)
    
    # === ì„¸ ë²ˆì§¸ í–‰: Linear ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ===
    # 7. Power Spectrogram
    ax = axes[2, 0]
    fbins, T = P.shape
    f_axis = np.arange(fbins) * SR / (2 * fbins)
    t_axis = np.arange(T) * HOP / SR
    im = ax.imshow(to_np(torch.log10(P + 1e-10)), aspect='auto', origin='lower', 
                   extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]], cmap='viridis')
    ax.axvspan(s*HOP/SR, e*HOP/SR, alpha=0.3, color='red')
    ax.axvspan((s+core_s_rel)*HOP/SR, (s+core_e_rel)*HOP/SR, alpha=0.5, color='orange')
    ax.set_xlabel('Time (s)')
    ax.set_ylabel('Frequency (Hz)')
    ax.set_title('Power Spectrogram (log10)')
    plt.colorbar(im, ax=ax, label='log10(Power)')
    
    # 8. Generated Mask
    ax = axes[2, 1]
    im = ax.imshow(to_np(M_lin), aspect='auto', origin='lower',
                   extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]], cmap='hot')
    ax.axvspan(s*HOP/SR, e*HOP/SR, alpha=0.3, color='cyan')
    ax.axvspan((s+core_s_rel)*HOP/SR, (s+core_e_rel)*HOP/SR, alpha=0.5, color='yellow')
    ax.set_xlabel('Time (s)')
    ax.set_ylabel('Frequency (Hz)')
    ax.set_title('Generated Mask')
    plt.colorbar(im, ax=ax, label='Mask Value')
    
    # 9. Masked Spectrogram
    ax = axes[2, 2]
    masked_spec = P * M_lin
    im = ax.imshow(to_np(torch.log10(masked_spec + 1e-10)), aspect='auto', origin='lower',
                   extent=[t_axis[0], t_axis[-1], f_axis[0], f_axis[-1]], cmap='viridis')
    ax.set_xlabel('Time (s)')
    ax.set_ylabel('Frequency (Hz)')
    ax.set_title('Masked Spectrogram (log10)')
    plt.colorbar(im, ax=ax, label='log10(Power)')
    
    plt.tight_layout()
    plt.savefig(png, dpi=150, bbox_inches='tight')
    plt.close()

# =========================
# IO & Spectra
# =========================
def load_fixed_audio(path: str) -> np.ndarray:
    wav, sro = torchaudio.load(path)
    if wav.shape[0] > 1:
        wav = wav.mean(dim=0, keepdim=True)
    if sro != SR:
        wav = torchaudio.functional.resample(wav, sro, SR)
    wav = wav.squeeze().numpy().astype(np.float32)
    if len(wav) >= L_FIXED:
        return wav[:L_FIXED]
    out = np.zeros(L_FIXED, dtype=np.float32)
    out[:len(wav)] = wav
    return out

@torch.no_grad()
def stft_all(audio: np.ndarray, mel_fb_m2f: torch.Tensor):
    wav = torch.from_numpy(audio)
    st = torch.stft(wav, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN,
                    window=WINDOW, return_complex=True, center=True)
    mag = st.abs()
    P   = (mag * mag).clamp_min(EPS)
    phase = torch.angle(st)

    if mel_fb_m2f.shape[0] != N_MELS:
        mel_fb_m2f = mel_fb_m2f.T.contiguous()
    assert mel_fb_m2f.shape[0] == N_MELS and mel_fb_m2f.shape[1] == P.shape[0]
    mel_fb_m2f = mel_fb_m2f.to(P.dtype).to(P.device)
    mel_pow = (mel_fb_m2f @ P).clamp_min(EPS)
    return st, mag, P, phase, mel_pow

# =========================
# AST Attention (Time & Frequency)
# =========================
@torch.no_grad()
def ast_attention_freq_time(audio: np.ndarray, extractor, ast_model, T_out: int, F_out: int) -> Tuple[torch.Tensor, torch.Tensor, str, str, int, float]:
    """
    AST ì–´í…ì…˜ì—ì„œ ì‹œê°„ê³¼ ì£¼íŒŒìˆ˜ ì •ë³´ë¥¼ ëª¨ë‘ ì¶”ì¶œí•˜ê³  ë¶„ë¥˜ ê²°ê³¼ë„ í•¨ê»˜ ë°˜í™˜
    Returns: (time_attention, freq_attention, class_name, sound_type, class_id, confidence)
    """
    # 10ì´ˆë¡œ íŒ¨ë”©
    target_len = int(10.0 * SR)
    if len(audio) < target_len:
        audio_padded = np.zeros(target_len, dtype=np.float32)
        audio_padded[:len(audio)] = audio
    else:
        audio_padded = audio[:target_len]
    
    feat = extractor(audio_padded, sampling_rate=SR, return_tensors="pt")
    out = ast_model(input_values=feat["input_values"], output_attentions=True, return_dict=True)
    attns = out.attentions
    
    # ë¶„ë¥˜ ê²°ê³¼ ì¶”ì¶œ
    logits = out.logits
    probabilities = torch.softmax(logits, dim=-1)
    predicted_class_id = logits.argmax(dim=-1).item()
    confidence = probabilities[0, predicted_class_id].item()
    
    class_name = ast_model.config.id2label[predicted_class_id]
    sound_type = get_sound_type(predicted_class_id)
    
    if not attns or len(attns) == 0:
        return torch.ones(T_out) * 0.5, torch.ones(F_out) * 0.5, class_name, sound_type, predicted_class_id, confidence
    
    # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì–´í…ì…˜ ì‚¬ìš©
    A = attns[-1]  # [batch, heads, seq, seq]
    
    # CLS í† í°(0ë²ˆ)ì—ì„œ íŒ¨ì¹˜ë“¤(2ë²ˆë¶€í„°)ë¡œì˜ ì–´í…ì…˜
    cls_to_patches = A[0, :, 0, 2:].mean(dim=0)  # í—¤ë“œë“¤ í‰ê· 
    
    # ASTëŠ” 12(freq) x 101(time) íŒ¨ì¹˜ êµ¬ì¡°
    Fp, Tp = 12, 101
    expected_len = Fp * Tp
    
    if cls_to_patches.numel() != expected_len:
        actual_len = cls_to_patches.numel()
        if actual_len < expected_len:
            cls_to_patches = F.pad(cls_to_patches, (0, expected_len - actual_len))
        else:
            cls_to_patches = cls_to_patches[:expected_len]
    
    # 2D ë§µìœ¼ë¡œ ì¬êµ¬ì„±
    full_map = cls_to_patches.reshape(Fp, Tp)  # [12, 101]
    
    # ì‹œê°„ ì–´í…ì…˜ (ì£¼íŒŒìˆ˜ ì°¨ì›ìœ¼ë¡œ í‰ê· )
    time_attn = full_map.mean(dim=0)  # [101]
    time_attn_interp = F.interpolate(time_attn.view(1,1,-1), size=T_out, mode="linear", align_corners=False).view(-1)
    time_attn_smooth = smooth1d(time_attn_interp, SMOOTH_T)
    time_attn_norm = norm01(time_attn_smooth)
    
    # ì£¼íŒŒìˆ˜ ì–´í…ì…˜ (ì‹œê°„ ì°¨ì›ìœ¼ë¡œ í‰ê· )
    freq_attn = full_map.mean(dim=1)  # [12]
    freq_attn_interp = F.interpolate(freq_attn.view(1,1,-1), size=F_out, mode="linear", align_corners=False).view(-1)
    freq_attn_norm = norm01(freq_attn_interp)
    
    return time_attn_norm, freq_attn_norm, class_name, sound_type, predicted_class_id, confidence

# =========================
# Attention & Purity
# =========================
def purity_from_P(P: torch.Tensor) -> torch.Tensor:
    fbins, T = P.shape
    e = P.sum(dim=0); e_n = e / (e.max() + EPS)
    p = P / (P.sum(dim=0, keepdim=True) + EPS)
    H = -(p * (p + EPS).log()).sum(dim=0)
    Hn = H / np.log(max(2, fbins))
    pur = W_E * e_n + (1.0 - W_E) * (1.0 - Hn)
    return norm01(smooth1d(pur, SMOOTH_T))

def anchor_score(A_t: torch.Tensor, Pur: torch.Tensor) -> torch.Tensor:
    return norm01(smooth1d((A_t.clamp(0,1)**ALPHA_ATT) * (Pur.clamp(0,1)**BETA_PUR), SMOOTH_T))

def pick_anchor_region(score: torch.Tensor, La: int, core_pct: float) -> Tuple[int, int, int, int]:
    """
    Finds the highest score peak, creates an anchor around it, and then finds
    the core region within that anchor, also centered on the peak.
    Returns: (anchor_start, anchor_end, core_start_relative, core_end_relative)
    """
    T = score.numel()

    # 1. Find the index of the absolute highest score.
    peak_idx = int(torch.argmax(score).item())

    # 2. Calculate the anchor window centered on the peak.
    anchor_s = max(0, min(peak_idx - (La // 2), T - La))
    anchor_e = anchor_s + La

    # 3. Define the local score window within the anchor.
    local_score = score[anchor_s:anchor_e]
    
    # 4. Find the peak's index relative to the start of the anchor.
    peak_idx_rel = int(torch.argmax(local_score).item())

    # 5. Define the threshold for expanding the core.
    threshold = torch.quantile(local_score, core_pct)

    # 6. Expand left and right from the relative peak to define the core.
    core_s_rel = peak_idx_rel
    while core_s_rel > 0 and local_score[core_s_rel - 1] >= threshold:
        core_s_rel -= 1
        
    core_e_rel = peak_idx_rel
    while core_e_rel < La - 1 and local_score[core_e_rel + 1] >= threshold:
        core_e_rel += 1
    
    # End index is exclusive, so add 1
    core_e_rel += 1

    return anchor_s, anchor_e, core_s_rel, core_e_rel

# =========================
# Î© & Template Generation (Adaptive Strategy)
# =========================
def omega_support_with_ast_freq(Ablk: torch.Tensor, ast_freq_attn: torch.Tensor, strategy: str = "conservative") -> torch.Tensor:
    """
    ì ì‘ì  ì „ëµì— ë”°ë¥¸ AST ì£¼íŒŒìˆ˜ ì–´í…ì…˜ì„ ê³ ë ¤í•œ omega ì§€ì› ê³„ì‚°
    """
    # ì „ëµì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì„ íƒ
    if strategy == "conservative":
        omega_q = OMEGA_Q_CONSERVATIVE
        ast_freq_quantile = AST_FREQ_QUANTILE_CONSERVATIVE
    else:  # aggressive
        omega_q = OMEGA_Q_AGGRESSIVE
        ast_freq_quantile = AST_FREQ_QUANTILE_AGGRESSIVE
    
    # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°ëœ ë§ˆìŠ¤í¬
    med = Ablk.median(dim=1).values
    th = torch.quantile(med, omega_q)
    mask_energy = (med >= th).float()
    
    # AST ì£¼íŒŒìˆ˜ ì–´í…ì…˜ì—ì„œ ìƒìœ„ ì£¼íŒŒìˆ˜ë“¤ ì„ íƒ
    ast_freq_th = torch.quantile(ast_freq_attn, ast_freq_quantile)
    mask_ast_freq = (ast_freq_attn >= ast_freq_th).float()
    
    # ë‘ ë§ˆìŠ¤í¬ë¥¼ ê²°í•© (OR ì—°ì‚°)
    mask = torch.maximum(mask_energy, mask_ast_freq)
    
    # ê¸°ì¡´ íŒ½ì°½ ì—°ì‚°
    for _ in range(OMEGA_DIL):
        mask = torch.maximum(mask, torch.roll(mask, 1))
        mask = torch.maximum(mask, torch.roll(mask, -1))
    
    # ìµœì†Œ ë¹ˆ ìˆ˜ ë³´ì¥
    if int(mask.sum().item()) < OMEGA_MIN_BINS:
        order = torch.argsort(med, descending=True)
        need = OMEGA_MIN_BINS - int(mask.sum().item())
        take = order[:need]
        mask[take] = 1.0
    
    return mask

def template_from_anchor_block(Ablk: torch.Tensor, omega: torch.Tensor) -> torch.Tensor:
    om = omega.view(-1,1)
    w  = (Ablk * om).mean(dim=1) * omega
    w  = w / (w.sum() + EPS)
    w_sm = F.avg_pool1d(w.view(1,1,-1), kernel_size=3, stride=1, padding=1).view(-1)
    w   = (w_sm * omega); w = w / (w.sum() + EPS)
    return w

# =========================
# Presence gate & cosÎ©
# =========================
def presence_from_energy(Xmel: torch.Tensor, omega: torch.Tensor) -> torch.Tensor:
    om = omega.view(-1,1)
    e_omega = (Xmel * om).sum(dim=0)
    e_omega = smooth1d(e_omega, PRES_SMOOTH_T)
    thr = torch.quantile(e_omega, PRES_Q)
    thr = torch.clamp(thr, min=1e-10)
    return (e_omega > thr).float()

def amplitude_raw(Xmel: torch.Tensor, w_bar: torch.Tensor, omega: torch.Tensor) -> torch.Tensor:
    om = omega.view(-1,1)
    Xo = Xmel * om
    denom = (w_bar*w_bar).sum() + EPS
    a_raw = (w_bar.view(1,-1) @ Xo).view(-1) / denom
    return a_raw.clamp_min(0.0)

def cos_similarity_over_omega(Xmel: torch.Tensor, w_bar: torch.Tensor, omega: torch.Tensor, g_pres: torch.Tensor):
    om = omega.view(-1,1)
    Xo = Xmel * om
    wn = (w_bar * omega); wn = wn / (wn.norm(p=2) + 1e-8)
    Xn = Xo / (Xo.norm(p=2, dim=0, keepdim=True) + 1e-8)
    cos_raw = (wn.view(-1,1) * Xn).sum(dim=0).clamp(0,1)
    return cos_raw * g_pres

# =========================
# Adaptive Masking Strategy
# =========================
def adaptive_masking_strategy(Xmel: torch.Tensor, w_bar: torch.Tensor, omega: torch.Tensor, 
                           ast_freq_attn: torch.Tensor, P: torch.Tensor, mel_fb_m2f: torch.Tensor,
                           s: int, e: int, strategy: str = "conservative") -> torch.Tensor:
    """
    ì ì‘ì  ë§ˆìŠ¤í‚¹ ì „ëµ: ë³´ìˆ˜ì /ê³µê²©ì  ëª¨ë“œì— ë”°ë¥¸ ë™ì  ë§ˆìŠ¤í¬ ìƒì„±
    """
    fbins, T = P.shape
    
    # 1. ê¸°ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    g_pres = presence_from_energy(Xmel, omega)
    cos_t_raw = cos_similarity_over_omega(Xmel, w_bar, omega, g_pres)
    
    # 2. ì „ëµì— ë”°ë¥¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì²˜ë¦¬
    if strategy == "conservative":
        # ë³´ìˆ˜ì  ë°©ì‹: ì œê³±ìœ¼ë¡œ ì•½í™”
        cos_processed = cos_t_raw ** 2
    else:  # aggressive
        # ê³µê²©ì  ë°©ì‹: ì‹œê·¸ëª¨ì´ë“œ ì ìš©
        cos_processed = torch.sigmoid(MASK_SIGMOID_SLOPE * (cos_t_raw - MASK_SIGMOID_CENTER))
    
    # 3. ì£¼íŒŒìˆ˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
    # Linear ë„ë©”ì¸ì—ì„œ ì§ì ‘ ê³„ì‚°
    omega_lin = ((mel_fb_m2f.T @ omega).clamp_min(0.0) > 1e-12).float()
    
    # ì•µì»¤ ì˜ì—­ì˜ ìƒìœ„ 20% ì§„í­ ì£¼íŒŒìˆ˜ ì„ íƒ
    anchor_spec = P[:, s:e]
    anchor_max_amp = anchor_spec.max(dim=1).values
    amp_threshold = torch.quantile(anchor_max_amp, 0.8)
    high_amp_mask_lin = (anchor_max_amp >= amp_threshold).float()
    
    # AST ì£¼íŒŒìˆ˜ ì–´í…ì…˜ì„ Linear ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜
    ast_freq_threshold = torch.quantile(ast_freq_attn, 0.4 if strategy == "conservative" else 0.2)
    ast_active_mask_mel = (ast_freq_attn >= ast_freq_threshold).float()
    ast_active_mask_lin = ((mel_fb_m2f.T @ ast_active_mask_mel).clamp_min(0.0) > 0.1).float()
    
    # ì£¼íŒŒìˆ˜ ê°€ì¤‘ì¹˜ ê²°í•©
    freq_boost_mask = torch.maximum(high_amp_mask_lin, ast_active_mask_lin)
    
    # 4. ì „ëµì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì ìš©
    if strategy == "conservative":
        # ë³´ìˆ˜ì  ë°©ì‹: 2ë°° ê°€ì¤‘ì¹˜
        freq_weight = 1.0 + freq_boost_mask  # [1.0, 2.0]
    else:  # aggressive
        # ê³µê²©ì  ë°©ì‹: 30% ê°€ì¤‘ì¹˜
        freq_weight = 1.0 + 0.3 * freq_boost_mask  # [1.0, 1.3]
    
    # 5. ê¸°ë³¸ ë§ˆìŠ¤í¬ ê³„ì‚°
    M_base = omega_lin.view(-1, 1) * cos_processed.view(1, -1)
    
    # 6. ì£¼íŒŒìˆ˜ ê°€ì¤‘ì¹˜ ì ìš©
    M_weighted = M_base * freq_weight.view(-1, 1)
    
    # 7. ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì œí•œ
    spec_magnitude = P.sqrt()
    M_lin = torch.minimum(M_weighted, spec_magnitude)
    
    return M_lin

def adaptive_strategy_selection(prev_energy_ratio: float, pass_idx: int) -> str:
    """
    ì´ì „ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì‘ì  ì „ëµ ì„ íƒ
    """
    if not USE_ADAPTIVE_STRATEGY:
        return "conservative"  # ê¸°ë³¸ê°’
    
    # ì²« ë²ˆì§¸ íŒ¨ìŠ¤ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ì‹œì‘
    if pass_idx == 0:
        return "conservative"
    
    # ì´ì „ì— ì—ë„ˆì§€ ë³´ì¡´ ë¬¸ì œê°€ ìˆì—ˆìœ¼ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ
    if prev_energy_ratio > 2.0:
        return "conservative"
    
    # ì—ë„ˆì§€ê°€ ë„ˆë¬´ ì ê²Œ ì¶”ì¶œë˜ì—ˆìœ¼ë©´ ê³µê²©ì ìœ¼ë¡œ
    if prev_energy_ratio < 1.2:
        return "aggressive"
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ë³´ìˆ˜ì  ì „ëµ ìœ ì§€
    return "conservative"

# =========================
# Single Pass Processing
# =========================
def single_pass(audio: np.ndarray, extractor, ast_model,
                mel_fb_m2f: torch.Tensor,
                used_mask_prev: Optional[torch.Tensor],
                prev_anchors: List[Tuple[float,float,torch.Tensor,torch.Tensor]],
                pass_idx: int, out_dir: Optional[str], prev_energy_ratio: float = 1.0,
                separated_time_regions: List[dict] = None):

    t0 = time.time()
    st, mag, P, phase, Xmel = stft_all(audio, mel_fb_m2f)
    fbins, T = P.shape
    La = int(round(ANCHOR_SEC * SR / HOP))

    # ì ì‘ì  ì „ëµ ì„ íƒ
    strategy = adaptive_strategy_selection(prev_energy_ratio, pass_idx)
    print(f"  ğŸ¯ Strategy: {strategy}")

    # ì´ì „ì— ë¶„ë¦¬ëœ ì‹œê°„ëŒ€ì˜ ì—ë„ˆì§€ ì–µì œ (AST ì¶”ë¡  ì „ì— ì ìš©)
    audio_for_ast = audio  # ASTìš© ì˜¤ë””ì˜¤ (ê¸°ë³¸ê°’: ì›ë³¸)
    if separated_time_regions and len(separated_time_regions) > 0:
        print(f"  ğŸ”‡ Suppressing energy in {len(separated_time_regions)} previously separated time regions")
        for region in separated_time_regions:
            time_mask = region['time_mask']
            class_name_prev = region['class_name']
            confidence_prev = region['confidence']
            
            # ì‹œê°„ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì •
            if time_mask.shape[0] != T:
                time_mask = align_len_1d(time_mask, T, device=P.device, mode="linear")
            
            # ì—ë„ˆì§€ ì–µì œ (2%ë§Œ ë‚¨ê¸°ê¸°)
            suppression_factor = 0.98  # 98% ì–µì œí•˜ì—¬ 2%ë§Œ ë‚¨ê¹€
            P_suppressed = P * (1.0 - time_mask * suppression_factor)
            P = P_suppressed
            
            print(f"    ğŸ“‰ Suppressed {class_name_prev} (conf: {confidence_prev:.3f}) to 2% (factor: {suppression_factor:.3f})")
        
        # ì–µì œëœ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜í•˜ì—¬ AST ëª¨ë¸ì— ì „ë‹¬
        print(f"  ğŸ”„ Converting suppressed spectrogram back to audio for AST inference")
        mag_suppressed = torch.sqrt(P)  # Powerì—ì„œ Magnitudeë¡œ ë³€í™˜
        stft_suppressed = mag_suppressed * torch.exp(1j * phase)  # ë³µì†Œìˆ˜ STFT ì¬êµ¬ì„±
        audio_for_ast = torch.istft(stft_suppressed, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN, 
                                   window=WINDOW, center=True, length=L_FIXED).detach().cpu().numpy()

    # ASTì—ì„œ ì‹œê°„ê³¼ ì£¼íŒŒìˆ˜ ì–´í…ì…˜ ëª¨ë‘ ì¶”ì¶œ (ë¶„ë¥˜ ê²°ê³¼ë„ í•¨ê»˜) - ì–µì œëœ ì˜¤ë””ì˜¤ ì‚¬ìš©
    A_t, ast_freq_attn, class_name, sound_type, class_id, confidence = ast_attention_freq_time(audio_for_ast, extractor, ast_model, T, N_MELS)
    Pur = purity_from_P(P)
    Sc = anchor_score(A_t, Pur)

    # Suppress used frames
    if used_mask_prev is not None:
        um = align_len_1d(used_mask_prev, T, device=Sc.device, mode="linear")
        k = int(round((USED_DILATE_MS/1000.0)*SR/HOP)); k = ensure_odd(max(1,k))
        ker = torch.ones(k, device=Sc.device)/k
        um = (F.conv1d(um.view(1,1,-1), ker.view(1,1,-1), padding=k//2).view(-1) > 0.2).float()
        Sc = Sc * (1 - 0.85 * um)

    # Enhanced suppression of previous anchors
    for (sa, ea, prev_w, prev_omega) in prev_anchors:
        ca = int(((sa+ea)/2) * SR / HOP)
        ca = max(0, min(T-1, ca))
        sigma = int(round((ANCHOR_SUPPRESS_MS/1000.0)*SR/HOP))
        idx = torch.arange(T, device=Sc.device) - ca
        Sc = Sc * (1 - ANCHOR_SUPPRESS_BASE * torch.exp(-(idx**2)/(2*(sigma**2)+1e-8)))
        core_s = max(0, ca - La//2); core_e = min(T, ca + La//2)
        Sc[core_s:core_e] *= 0.2
    
    # Pick anchor and core regions, centered on the peak score
    s, e, core_s_rel, core_e_rel = pick_anchor_region(Sc, La, TOP_PCT_CORE_IN_ANCHOR)
    
    # Create anchor block (Ablk) based on the core indices
    Ablk = Xmel[:, s:e].clone()
    if core_s_rel > 0:  Ablk[:, :core_s_rel] = 0
    if core_e_rel < La: Ablk[:, core_e_rel:] = 0

    # AST ì£¼íŒŒìˆ˜ ì–´í…ì…˜ì„ ê³ ë ¤í•œ Î© ê³„ì‚°
    omega = omega_support_with_ast_freq(Ablk, ast_freq_attn, strategy)
    w_bar = template_from_anchor_block(Ablk, omega)
    
    # ì ì‘ì  ë§ˆìŠ¤í‚¹ ì „ëµ ì ìš©
    M_lin = adaptive_masking_strategy(Xmel, w_bar, omega, ast_freq_attn, P, mel_fb_m2f, s, e, strategy)
    
    # Subtraction in the complex STFT domain for precision
    stft_full = st
    
    # ë§ˆìŠ¤í¬ë¥¼ ì§„í­ì—ë§Œ ì ìš©í•˜ê³  ìœ„ìƒì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    mag_masked = M_lin * mag  # ì§„í­ì— ë§ˆìŠ¤í¬ ì ìš©
    stft_src = mag_masked * torch.exp(1j * phase)  # ë³µì†Œìˆ˜ STFT ì¬êµ¬ì„±
    
    # ì”ì—¬ë¬¼ ê³„ì‚°: ì§„í­ ê¸°ë°˜ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ê³„ì‚° (ì—ë„ˆì§€ ë³´ì¡´)
    mag_residual = torch.maximum(mag - mag_masked, torch.zeros_like(mag))
    stft_res = mag_residual * torch.exp(1j * phase)  # ì”ì—¬ë¬¼ ë³µì†Œìˆ˜ STFT ì¬êµ¬ì„±
    
    # ë””ë²„ê¹…: ëº„ì…ˆ ê²°ê³¼ ê²€ì¦
    src_energy = torch.sum(torch.abs(stft_src)**2).item()
    res_energy = torch.sum(torch.abs(stft_res)**2).item()
    orig_energy = torch.sum(torch.abs(stft_full)**2).item()
    total_energy = src_energy + res_energy
    
    print(f"  ğŸ” Energy: Original={orig_energy:.6f}, Source={src_energy:.6f}, Residual={res_energy:.6f}")
    print(f"  ğŸ” Energy ratio: Src/Orig={src_energy/(orig_energy+1e-8):.3f}, Res/Orig={res_energy/(orig_energy+1e-8):.3f}")
    print(f"  ğŸ” Energy conservation: Total/Orig={total_energy/(orig_energy+1e-8):.3f}")
    
    # ì—ë„ˆì§€ ë³´ì¡´ ê²€ì¦ ë° ì •ê·œí™”
    energy_ratio = total_energy / (orig_energy + 1e-8)
    if energy_ratio > 1.1:  # ì´ ì—ë„ˆì§€ê°€ ì›ë³¸ì˜ 110%ë¥¼ ë„˜ìœ¼ë©´
        print(f"  âš ï¸ WARNING: Energy not conserved! Total/Orig={energy_ratio:.3f}")
        # ì—ë„ˆì§€ ì •ê·œí™”
        scale_factor = orig_energy / (total_energy + 1e-8)
        scale_tensor = torch.tensor(scale_factor, device=stft_src.device, dtype=stft_src.dtype)
        stft_src = stft_src * torch.sqrt(scale_tensor)
        stft_res = stft_res * torch.sqrt(scale_tensor)
        print(f"  ğŸ”§ Scaled energies by factor {scale_factor:.3f}")

    # Reconstruct both source and residual
    src_amp = torch.istft(stft_src, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN, 
                         window=WINDOW, center=True, length=L_FIXED).detach().cpu().numpy()
    res = torch.istft(stft_res, n_fft=N_FFT, hop_length=HOP, win_length=WINLEN, 
                      window=WINDOW, center=True, length=L_FIXED).detach().cpu().numpy()

    # ER calculation
    e_src = float(np.sum(src_amp**2)); e_res = float(np.sum(res**2))
    er = e_src / (e_src + e_res + 1e-12)

    # Used-frame mask for next pass
    r_t = (M_lin * P).sum(dim=0) / (P.sum(dim=0) + 1e-8)
    used_mask = (r_t >= USED_THRESHOLD).float()

    elapsed = time.time() - t0
    
    # Global purity calculation
    g_pres = presence_from_energy(Xmel, omega)
    cos_t_raw = cos_similarity_over_omega(Xmel, w_bar, omega, g_pres)
    global_purity = cos_t_raw.mean().item()
    
    # ì•µì»¤ ì¤‘ì‹¬ ê³„ì‚° (ë¶„ë¦¬ ê±´ë„ˆë›°ê¸° ë¡œì§ì—ì„œ ì‚¬ìš©)
    ca = (s + e) // 2
    
    # ë¶„ë¦¬ ê±´ë„ˆë›°ê¸° ì¡°ê±´ í™•ì¸
    if should_skip_separation(confidence, Pur[ca], class_id):
        print(f"  âš¡ High confidence & purity detected! Skipping separation...")
        print(f"  ğŸ“Š Confidence: {confidence:.3f} (â‰¥0.8), Purity: {Pur[ca]:.3f} (â‰¥0.7)")
        
        # ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ)
        src_amp = audio.copy()
        res = np.zeros_like(audio)
        er = 1.0  # ì „ì²´ê°€ ì†ŒìŠ¤ë¡œ ê°„ì£¼
        
        # ì •ë³´ ë°˜í™˜
        info = {
            "er": er,
            "elapsed": elapsed,
            "anchor": (s*HOP/SR, e*HOP/SR),
            "core": ((s+core_s_rel)*HOP/SR, (s+core_e_rel)*HOP/SR),
            "quality": 1.0,  # ë¶„ë¦¬í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ìµœê³  í’ˆì§ˆ
            "w_bar": w_bar,
            "omega": omega,
            "sound_type": sound_type,
            "sound_detail": class_name,
            "class_id": class_id,
            "confidence": confidence,
            "purity": Pur[ca],
            "separation_skipped": True,
            "strategy": strategy,
            "energy_ratio": 1.0,
            "db_min": db_min,
            "db_max": db_max,
            "db_mean": db_mean
        }
        
        return src_amp, res, er, None, info
    
    # Debug plot generation
    if out_dir is not None:
        # í•„ìš”í•œ ë³€ìˆ˜ë“¤ ê³„ì‚°
        a_raw = amplitude_raw(Xmel, w_bar, omega)
        cos_t_raw_debug = cos_similarity_over_omega(Xmel, w_bar, omega, presence_from_energy(Xmel, omega))
        C_t = cos_t_raw_debug  # ë³„ì¹­
        
        # full_mapì€ AST ì–´í…ì…˜ì—ì„œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        full_map = torch.zeros(12, 101)  # ê¸°ë³¸ í¬ê¸°
        
        png = os.path.join(out_dir, f"debug_pass_{pass_idx+1}.png")
        debug_plot(pass_idx, Sc, a_raw, cos_t_raw_debug, C_t, P, M_lin, full_map,
                  s, e, core_s_rel, core_e_rel, ast_freq_attn, src_amp, res, png,
                  title=f"Pass {pass_idx+1} | Strategy: {strategy} | anchor {s*HOP/SR:.2f}-{e*HOP/SR:.2f}s | ER={er*100:.1f}% [Adaptive Mask]",
                  original_audio=audio, global_confidence=confidence, global_purity=global_purity)
    
    # ë°ì‹œë²¨ ê³„ì‚°
    db_min, db_max, db_mean = calculate_decibel(src_amp)
    
    # ë¶„ë¦¬ëœ ì†ŒìŠ¤ì˜ ì‹œê°„ëŒ€ ì •ë³´ ê³„ì‚° (ë‹¤ìŒ íŒ¨ìŠ¤ì—ì„œ ì—ë„ˆì§€ ì–µì œìš©)
    src_time_mask = (M_lin.sum(dim=0) > 1e-6).float()  # ë¶„ë¦¬ëœ ì‹œê°„ í”„ë ˆì„ë“¤
    src_time_indices = torch.where(src_time_mask > 0)[0]  # ë¶„ë¦¬ëœ ì‹œê°„ ì¸ë±ìŠ¤ë“¤
    
    info = {
        "er": er,
        "elapsed": elapsed,
        "anchor": (s*HOP/SR, e*HOP/SR),
        "core": ((s+core_s_rel)*HOP/SR, (s+core_e_rel)*HOP/SR),
        "quality": float(M_lin.mean().item()),
        "w_bar": w_bar,
        "omega": omega,
        "stopped": False,
        "strategy": strategy,
        "energy_ratio": energy_ratio,
        "class_name": class_name,
        "sound_type": sound_type,
        "class_id": class_id,
        "confidence": confidence,
        "db_min": db_min,
        "db_max": db_max,
        "db_mean": db_mean,
        "src_time_mask": src_time_mask,  # ë¶„ë¦¬ëœ ì‹œê°„ ë§ˆìŠ¤í¬
        "src_time_indices": src_time_indices  # ë¶„ë¦¬ëœ ì‹œê°„ ì¸ë±ìŠ¤
    }
    return src_amp, res, er, used_mask, info

# =========================
# Main Processing Pipeline
# =========================
def main():
    ap = argparse.ArgumentParser(description="AST-guided Source Separator")
    ap.add_argument("--input", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--passes", type=int, default=MAX_PASSES)
    ap.add_argument("--no-debug", action="store_true")
    ap.add_argument("--strategy", choices=["conservative", "aggressive", "adaptive"], default="adaptive")
    args = ap.parse_args()

    # ì „ì—­ ì „ëµ ì„¤ì •
    global USE_ADAPTIVE_STRATEGY
    if args.strategy == "adaptive":
        USE_ADAPTIVE_STRATEGY = True
    else:
        USE_ADAPTIVE_STRATEGY = False

    os.makedirs(args.output, exist_ok=True)

    audio0 = load_fixed_audio(args.input)
    print(f"\n{'='*64}\nğŸµ AST-guided Source Separator\n{'='*64}")
    print(f"Input: fixed {WIN_SEC:.3f}s, Anchor: {ANCHOR_SEC:.3f}s, Passes: {args.passes}")
    print(f"Strategy: {args.strategy}")
    print(f"Features: Adaptive Masking, Energy Conservation, Classification, Decibel Analysis")

    # Mel FB: [F,M] -> [M,F]
    fbins = N_FFT//2 + 1
    mel_fb_f2m = torchaudio.functional.melscale_fbanks(
        n_freqs=fbins, f_min=0.0, f_max=SR/2, n_mels=N_MELS,
        sample_rate=SR, norm="slaney"
    )
    mel_fb_m2f = mel_fb_f2m.T.contiguous()

    # AST (CPU)
    extractor = ASTFeatureExtractor.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593")
    ast_model = ASTForAudioClassification.from_pretrained(
        "MIT/ast-finetuned-audioset-10-10-0.4593",
        attn_implementation="eager"
    ).eval()

    cur = audio0.copy()
    used_mask_prev = None
    prev_anchors: List[Tuple[float,float,torch.Tensor,torch.Tensor]] = []
    total_t0 = time.time()
    saved = 0
    prev_energy_ratio = 1.0
    separated_time_regions = []  # ì´ì „ì— ë¶„ë¦¬ëœ ì‹œê°„ëŒ€ ì •ë³´ ì €ì¥

    for i in range(max(1, args.passes)):
        print(f"\nâ–¶ Pass {i+1}/{args.passes}")
        result = single_pass(
            cur, extractor, ast_model, mel_fb_m2f,
            used_mask_prev, prev_anchors,
            pass_idx=i, out_dir=None if args.no_debug else args.output,
            prev_energy_ratio=prev_energy_ratio,
            separated_time_regions=separated_time_regions
        )
        
        if result[0] is None:
            info = result[4]
            reason = info.get("reason", "stopped")
            print(f"  â¹ï¸ Stopped: {reason}")
            break
        
        src, res, er, used_mask_prev, info = result
        
        # ë¶„ë¥˜ ì •ë³´ ì¶œë ¥
        class_name = info['class_name']
        sound_type = info['sound_type']
        class_id = info['class_id']
        confidence = info['confidence']
        strategy = info['strategy']
        energy_ratio = info['energy_ratio']
        db_min, db_max, db_mean = info['db_min'], info['db_max'], info['db_mean']
        
        # ë¶„ë¦¬ ê±´ë„ˆë›°ê¸° ì—¬ë¶€ í™•ì¸
        separation_skipped = info.get('separation_skipped', False)
        
        if separation_skipped:
            print(f"â±ï¸ pass{i+1}: {info['elapsed']:.3f}s | anchor {info['anchor'][0]:.2f}-{info['anchor'][1]:.2f}s | SKIPPED")
            print(f"  ğŸ¯ Strategy: {strategy} | Separation: SKIPPED (High confidence & purity)")
            print(f"  ğŸµ Class: {class_name} (ID: {class_id}) | Type: {sound_type} | Confidence: {confidence:.3f}")
            print(f"  ğŸ”Š Decibel: min={db_min:.1f}dB, max={db_max:.1f}dB, mean={db_mean:.1f}dB")
        else:
            print(f"â±ï¸ pass{i+1}: {info['elapsed']:.3f}s | anchor {info['anchor'][0]:.2f}-{info['anchor'][1]:.2f}s"
                  f" | core {info['core'][0]:.2f}-{info['core'][1]:.2f}s | ER={er*100:.1f}%")
            print(f"  ğŸ¯ Strategy: {strategy} | Energy Ratio: {energy_ratio:.3f}")
            print(f"  ğŸµ Class: {class_name} (ID: {class_id}) | Type: {sound_type} | Confidence: {confidence:.3f}")
            print(f"  ğŸ”Š Decibel: min={db_min:.1f}dB, max={db_max:.1f}dB, mean={db_mean:.1f}dB")

        if er < MIN_ERATIO:
            print("  âš ï¸ Too little energy; stopping.")
            break
        
        # ë¶„ë¦¬ëœ ì‹œê°„ëŒ€ ì •ë³´ ìˆ˜ì§‘ (ë‹¤ìŒ íŒ¨ìŠ¤ì—ì„œ ì—ë„ˆì§€ ì–µì œìš©) - ë¶„ë¦¬ ê±´ë„ˆë›°ê¸° ì‹œì—ëŠ” ìˆ˜ì§‘í•˜ì§€ ì•ŠìŒ
        if not separation_skipped:
            src_time_mask = info.get('src_time_mask')
            if src_time_mask is not None:
                separated_time_regions.append({
                    'pass': i + 1,
                    'class_name': class_name,
                    'time_mask': src_time_mask,
                    'time_indices': info.get('src_time_indices', []),
                    'confidence': confidence
                })
                print(f"  ğŸ“Š Collected {len(separated_time_regions)} separated time regions for energy suppression")

        # Peak Normalization for clear output
        peak = np.max(np.abs(src))
        if peak > 1e-8:
            gain = NORMALIZE_TARGET_PEAK / peak
            src = src * gain
            src = np.clip(src, -1.0, 1.0)

        # í´ë˜ìŠ¤ëª…ì„ í¬í•¨í•œ íŒŒì¼ëª… ìƒì„±
        safe_class_name = "".join(c for c in class_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_class_name = safe_class_name.replace(' ', '_')
        out_src = os.path.join(args.output, f"{i+1:02d}_{safe_class_name}_{sound_type}.wav")
        torchaudio.save(out_src, torch.from_numpy(src).unsqueeze(0), SR)
        print(f"    âœ… Saved (Normalized): {out_src}")

        cur = res
        prev_anchors.append((info["anchor"][0], info["anchor"][1], info["w_bar"], info["omega"]))
        prev_energy_ratio = energy_ratio
        saved += 1

    # Apply Hard Clipping to the final residual
    if RESIDUAL_CLIP_THR > 0:
        print(f"\nApplying residual clipping with threshold: {RESIDUAL_CLIP_THR}")
        cur[np.abs(cur) < RESIDUAL_CLIP_THR] = 0.0
    
    # Residual ë¶„ë¥˜ (ì‹ ë¢°ë„ 0.7 ê¸°ì¤€)
    print(f"\nğŸ” Classifying residual audio...")
    res_class_name, res_sound_type, res_class_id, res_confidence = classify_audio_segment(cur, extractor, ast_model)
    res_db_min, res_db_max, res_db_mean = calculate_decibel(cur)
    
    print(f"  ğŸµ Residual Class: {res_class_name} (ID: {res_class_id}) | Type: {res_sound_type} | Confidence: {res_confidence:.3f}")
    print(f"  ğŸ”Š Residual Decibel: min={res_db_min:.1f}dB, max={res_db_max:.1f}dB, mean={res_db_mean:.1f}dB")
    
    # ì‹ ë¢°ë„ 0.7 ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ëª… ê²°ì •
    if res_confidence >= 0.7:
        safe_res_class_name = "".join(c for c in res_class_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_res_class_name = safe_res_class_name.replace(' ', '_')
        out_res = os.path.join(args.output, f"00_{safe_res_class_name}_{res_sound_type}.wav")
        print(f"  âœ… High confidence ({res_confidence:.3f} â‰¥ 0.7), using class name: {res_class_name}")
    else:
        out_res = os.path.join(args.output, "00_residual.wav")
        print(f"  âš ï¸ Low confidence ({res_confidence:.3f} < 0.7), using generic name")
    
    torchaudio.save(out_res, torch.from_numpy(cur).unsqueeze(0), SR)

    total_elapsed = time.time() - total_t0
    print(f"\nğŸ’¾ Residual: {out_res}")
    print(f"â±ï¸ Total: {total_elapsed:.3f}s")
    print(f"âœ… Done. Separated: {saved}\n")

if __name__ == "__main__":
    main()
